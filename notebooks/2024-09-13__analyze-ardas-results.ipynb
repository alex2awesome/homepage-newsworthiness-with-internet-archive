{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc497f0-713e-460a-88dd-75c8e644d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f230e5c5-20cc-4758-b737-cf77c67467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the given results into a list of Python dictionaries\n",
    "results = [\n",
    "    {\n",
    "        \"model\": \"Bert-base-uncased\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.5905096660808435,\n",
    "        \"eval_f1\": 0.680161943319838,\n",
    "        \"eval_precision\": 0.801909307875895\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Bert-base-uncased\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 0.792411755559011,\n",
    "        \"eval_f1\": 0.79026130107777,\n",
    "        \"eval_precision\": 0.7881224868543149\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"distilbert/distilbert-base-uncased\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.6265022421524664,\n",
    "        \"eval_f1\": 0.7009507563404661,\n",
    "        \"eval_precision\": 0.7954791322666971\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"distilbert/distilbert-base-uncased\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 0.9287298067785873,\n",
    "        \"eval_f1\": 0.9309783687239531,\n",
    "        \"eval_precision\": 0.933237845149996\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"distilbert/distilbert-base-uncased\",\n",
    "        \"variable_analyzed\": \"Position x Size\",\n",
    "        \"eval_recall\": 0.6955544856363345,\n",
    "        \"eval_f1\": 0.7517127742606886,\n",
    "        \"eval_precision\": 0.8177358490566038\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Flan-t5-base\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.5449775784753363,\n",
    "        \"eval_f1\": 0.644841217202133,\n",
    "        \"eval_precision\": 0.7895147144806081\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Flan-t5-base\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 0.919662654418752,\n",
    "        \"eval_f1\": 0.9187895569620254,\n",
    "        \"eval_precision\": 0.9179181157129308\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Flan-t5-base\",\n",
    "        \"variable_analyzed\": \"Position x Size\",\n",
    "        \"eval_recall\": 0.6305167709837908,\n",
    "        \"eval_f1\": 0.7071502497412591,\n",
    "        \"eval_precision\": 0.80498924290544\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Flan-t5-Large\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.6405797101449275,\n",
    "        \"eval_f1\": 0.5230769230769231,\n",
    "        \"eval_precision\": 0.442\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Flan-t5-Large\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 0.17,\n",
    "        \"eval_f1\": 0.17364657814096016,\n",
    "        \"eval_precision\": 0.17745302713987474\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Roberta Base\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.26246636771300447,\n",
    "        \"eval_f1\": 0.36285298037878555,\n",
    "        \"eval_precision\": 0.5875916072683466\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Roberta Base\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 1.0,\n",
    "        \"eval_f1\": 0.6666666666666666,\n",
    "        \"eval_precision\": 0.5\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Roberta Large\",\n",
    "        \"variable_analyzed\": \"Position\",\n",
    "        \"eval_recall\": 0.439237668161435,\n",
    "        \"eval_f1\": 0.4927062374245473,\n",
    "        \"eval_precision\": 0.5609965635738832\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Roberta Large\",\n",
    "        \"variable_analyzed\": \"Size\",\n",
    "        \"eval_recall\": 0.8487092176116566,\n",
    "        \"eval_f1\": 0.8539160226276791,\n",
    "        \"eval_precision\": 0.8591871091871092\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Roberta Large\",\n",
    "        \"variable_analyzed\": \"Position x Size\",\n",
    "        \"eval_recall\": 0.3460519980741454,\n",
    "        \"eval_f1\": 0.47242153694473354,\n",
    "        \"eval_precision\": 0.7441760138050043\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bbcdf2-3310-4d94-8cc9-2ac97853da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbbeb1f-6eae-482d-b535-ec226c3291d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/qnyq7yzj0r328_7hnb7pgxth0000gp/T/ipykernel_13571/3479119505.py:1: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  results_df.loc[lambda df: df['eval_f1'] == df.groupby('model')['eval_f1'].transform(max)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>variable_analyzed</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bert-base-uncased</td>\n",
       "      <td>Size</td>\n",
       "      <td>0.792412</td>\n",
       "      <td>0.790261</td>\n",
       "      <td>0.788122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilbert-base-uncased</td>\n",
       "      <td>Size</td>\n",
       "      <td>0.928730</td>\n",
       "      <td>0.930978</td>\n",
       "      <td>0.933238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Flan-t5-base</td>\n",
       "      <td>Size</td>\n",
       "      <td>0.919663</td>\n",
       "      <td>0.918790</td>\n",
       "      <td>0.917918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Flan-t5-Large</td>\n",
       "      <td>Position</td>\n",
       "      <td>0.640580</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Roberta Base</td>\n",
       "      <td>Size</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Roberta Large</td>\n",
       "      <td>Size</td>\n",
       "      <td>0.848709</td>\n",
       "      <td>0.853916</td>\n",
       "      <td>0.859187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model variable_analyzed  eval_recall  \\\n",
       "1                    Bert-base-uncased              Size     0.792412   \n",
       "3   distilbert/distilbert-base-uncased              Size     0.928730   \n",
       "6                         Flan-t5-base              Size     0.919663   \n",
       "8                        Flan-t5-Large          Position     0.640580   \n",
       "11                        Roberta Base              Size     1.000000   \n",
       "13                       Roberta Large              Size     0.848709   \n",
       "\n",
       "     eval_f1  eval_precision  \n",
       "1   0.790261        0.788122  \n",
       "3   0.930978        0.933238  \n",
       "6   0.918790        0.917918  \n",
       "8   0.523077        0.442000  \n",
       "11  0.666667        0.500000  \n",
       "13  0.853916        0.859187  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[lambda df: df['eval_f1'] == df.groupby('model')['eval_f1'].transform(max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d0b694f-b15c-4bc4-a6fe-104a10d88533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/qnyq7yzj0r328_7hnb7pgxth0000gp/T/ipykernel_13571/2411902217.py:1: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
      "  results_df.loc[lambda df: df['eval_f1'] == df.groupby('variable_analyzed')['eval_f1'].transform(max)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>variable_analyzed</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert/distilbert-base-uncased</td>\n",
       "      <td>Position</td>\n",
       "      <td>0.626502</td>\n",
       "      <td>0.700951</td>\n",
       "      <td>0.795479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilbert-base-uncased</td>\n",
       "      <td>Size</td>\n",
       "      <td>0.928730</td>\n",
       "      <td>0.930978</td>\n",
       "      <td>0.933238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert/distilbert-base-uncased</td>\n",
       "      <td>Position x Size</td>\n",
       "      <td>0.695554</td>\n",
       "      <td>0.751713</td>\n",
       "      <td>0.817736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model variable_analyzed  eval_recall  \\\n",
       "2  distilbert/distilbert-base-uncased          Position     0.626502   \n",
       "3  distilbert/distilbert-base-uncased              Size     0.928730   \n",
       "4  distilbert/distilbert-base-uncased   Position x Size     0.695554   \n",
       "\n",
       "    eval_f1  eval_precision  \n",
       "2  0.700951        0.795479  \n",
       "3  0.930978        0.933238  \n",
       "4  0.751713        0.817736  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[lambda df: df['eval_f1'] == df.groupby('variable_analyzed')['eval_f1'].transform(max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1537fad8-4ac3-4c7f-b52a-e48d73417215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "Flan-t5-Large                         0.173647\n",
       "Roberta Base                          0.362853\n",
       "Roberta Large                         0.472422\n",
       "Flan-t5-base                          0.644841\n",
       "Bert-base-uncased                     0.680162\n",
       "distilbert/distilbert-base-uncased    0.700951\n",
       "Name: eval_f1, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby('model')['eval_f1'].min().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86847c77-c7db-49ab-9f2e-450ae9e4a4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61265726-b30b-4373-b0a8-b46d23041c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff05f-8c8e-4a39-ab4a-6314462d7355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e5abf-b07f-449a-ab22-36f31fa0314f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f99ed711-c4f7-493e-a6e7-6c246d915cb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m cannot_link \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)]  \u001b[38;5;66;03m# Points 2 and 3 cannot be in the same cluster\u001b[39;00m\n\u001b[1;32m     96\u001b[0m adaptive_pckmeans \u001b[38;5;241m=\u001b[39m AdaptivePCKMeans(max_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m \u001b[43madaptive_pckmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmust_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcannot_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcannot_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Cluster Centers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, adaptive_pckmeans\u001b[38;5;241m.\u001b[39mbest_centers_)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, adaptive_pckmeans\u001b[38;5;241m.\u001b[39mbest_labels_)\n",
      "Cell \u001b[0;32mIn[24], line 40\u001b[0m, in \u001b[0;36mAdaptivePCKMeans.fit\u001b[0;34m(self, X, must_link, cannot_link)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_clusters \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_clusters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_clusters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     pckmeans \u001b[38;5;241m=\u001b[39m PCKMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtolerance)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mpckmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmust_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcannot_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcannot_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     labels \u001b[38;5;241m=\u001b[39m pckmeans\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Calculate the silhouette score to evaluate the clustering quality\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 78\u001b[0m, in \u001b[0;36mPCKMeans.fit\u001b[0;34m(self, X, must_link, cannot_link)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m     77\u001b[0m     old_centers \u001b[38;5;241m=\u001b[39m centers\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 78\u001b[0m     centers \u001b[38;5;241m=\u001b[39m \u001b[43mapply_constraints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcannot_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Step 3: Check convergence\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(old_centers \u001b[38;5;241m-\u001b[39m centers) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtolerance:\n",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m, in \u001b[0;36mapply_constraints\u001b[0;34m(centers, X, must_link, cannot_link)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Check cannot-link constraints\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m cannot_link:\n\u001b[0;32m---> 16\u001b[0m     dist_ij \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mcenters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m centers[j])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist_ij \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-3\u001b[39m:  \u001b[38;5;66;03m# Threshold to ensure separation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         centers[i] \u001b[38;5;241m=\u001b[39m centers[i] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, centers\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Helper function to enforce pairwise constraints\n",
    "def apply_constraints(centers, X, must_link, cannot_link):\n",
    "    # Check must-link constraints\n",
    "    for i, j in must_link:\n",
    "        X_mean = (X[i] + X[j]) / 2\n",
    "        centers[i] = X_mean\n",
    "        centers[j] = X_mean\n",
    "\n",
    "    # Check cannot-link constraints\n",
    "    for i, j in cannot_link:\n",
    "        dist_ij = np.linalg.norm(centers[i] - centers[j])\n",
    "        if dist_ij < 1e-3:  # Threshold to ensure separation\n",
    "            centers[i] = centers[i] + np.random.uniform(0.1, 0.5, centers.shape[1])\n",
    "            centers[j] = centers[j] - np.random.uniform(0.1, 0.5, centers.shape[1])\n",
    "\n",
    "    return centers\n",
    "\n",
    "# PCKMeans with adaptive cluster number learning\n",
    "class AdaptivePCKMeans:\n",
    "    def __init__(self, max_clusters=10, min_clusters=2, max_iter=100, tolerance=1e-4):\n",
    "        self.max_clusters = max_clusters\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.best_n_clusters_ = None\n",
    "        self.best_centers_ = None\n",
    "        self.best_labels_ = None\n",
    "\n",
    "    def fit(self, X, must_link=[], cannot_link=[]):\n",
    "        best_silhouette = -1  # Initialize to the worst silhouette score\n",
    "\n",
    "        # Try different numbers of clusters and evaluate performance\n",
    "        for n_clusters in range(self.min_clusters, self.max_clusters + 1):\n",
    "            pckmeans = PCKMeans(n_clusters=n_clusters, max_iter=self.max_iter, tolerance=self.tolerance)\n",
    "            pckmeans.fit(X, must_link=must_link, cannot_link=cannot_link)\n",
    "            labels = pckmeans.predict(X)\n",
    "            \n",
    "            # Calculate the silhouette score to evaluate the clustering quality\n",
    "            score = silhouette_score(X, labels)\n",
    "\n",
    "            # If the silhouette score is better, we update the best solution\n",
    "            if score > best_silhouette:\n",
    "                best_silhouette = score\n",
    "                self.best_n_clusters_ = n_clusters\n",
    "                self.best_centers_ = pckmeans.centers_\n",
    "                self.best_labels_ = labels\n",
    "\n",
    "        print(f\"Best number of clusters: {self.best_n_clusters_}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.best_centers_ is None:\n",
    "            raise ValueError(\"The model has not been fitted yet.\")\n",
    "        # Assign points to the nearest cluster center\n",
    "        return np.argmin(cdist(X, self.best_centers_), axis=1)\n",
    "\n",
    "# Reusing PCKMeans class from previous implementation\n",
    "class PCKMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=100, tolerance=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.centers_ = None\n",
    "\n",
    "    def fit(self, X, must_link=[], cannot_link=[]):\n",
    "        # Step 1: Initialize KMeans\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, max_iter=self.max_iter, tol=self.tolerance, init='k-means++')\n",
    "        kmeans.fit(X)\n",
    "        centers = kmeans.cluster_centers_\n",
    "\n",
    "        # Step 2: Apply pairwise constraints\n",
    "        for _ in range(self.max_iter):\n",
    "            old_centers = centers.copy()\n",
    "            centers = apply_constraints(centers, X, must_link, cannot_link)\n",
    "\n",
    "            # Step 3: Check convergence\n",
    "            if np.linalg.norm(old_centers - centers) < self.tolerance:\n",
    "                break\n",
    "\n",
    "        self.centers_ = centers\n",
    "        self.labels_ = self.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Step 4: Assign points to the nearest cluster center\n",
    "        return np.argmin(cdist(X, self.centers_), axis=1)\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "must_link = [(0, 1)]  # Points 0 and 1 must be in the same cluster\n",
    "cannot_link = [(2, 3)]  # Points 2 and 3 cannot be in the same cluster\n",
    "\n",
    "adaptive_pckmeans = AdaptivePCKMeans(max_clusters=4)\n",
    "adaptive_pckmeans.fit(X, must_link=must_link, cannot_link=cannot_link)\n",
    "print(\"Best Cluster Centers:\", adaptive_pckmeans.best_centers_)\n",
    "print(\"Best Labels:\", adaptive_pckmeans.best_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3904fb3-e47c-4867-a694-65bb044d2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class HierarchicalClusteringWithConstraints:\n",
    "    def __init__(self, must_link=None, similar=None):\n",
    "        \"\"\"\n",
    "        Initialize with optional pairwise constraints.\n",
    "        :param must_link: List of tuples (i, j) where i and j should be in the same cluster.\n",
    "        :param similar: List of tuples (i, j) where i and j should be in similar clusters (higher level).\n",
    "        \"\"\"\n",
    "        self.must_link = must_link if must_link is not None else []\n",
    "        self.similar = similar if similar is not None else []\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Perform hierarchical clustering with the given must-link and similar constraints.\n",
    "        :param X: Data matrix of shape (n_samples, n_features).\n",
    "        :return: A hierarchical clustering.\n",
    "        \"\"\"\n",
    "        # Step 1: Apply must-link constraints to form initial clusters\n",
    "        n_samples = X.shape[0]\n",
    "        adjacency_matrix = np.zeros((n_samples, n_samples))\n",
    "\n",
    "        # Apply must-link constraints\n",
    "        for i, j in self.must_link:\n",
    "            adjacency_matrix[i, j] = 1\n",
    "            adjacency_matrix[j, i] = 1\n",
    "        \n",
    "        # Find connected components from must-link constraints\n",
    "        _, labels = connected_components(csgraph=adjacency_matrix, directed=False)\n",
    "        \n",
    "        # Step 2: Create pairwise distances\n",
    "        distance_matrix = pairwise_distances(X)\n",
    "\n",
    "        # Modify distances for must-link constraints (force them to be 0)\n",
    "        for i, j in self.must_link:\n",
    "            distance_matrix[i, j] = 0\n",
    "            distance_matrix[j, i] = 0\n",
    "        \n",
    "        # Step 3: Apply hierarchical clustering with the distance matrix\n",
    "        # Start with the clusters formed by must-link constraints\n",
    "        agg_clustering = AgglomerativeClustering(\n",
    "            n_clusters=None, distance_threshold=0, metric='precomputed', linkage='average'\n",
    "        )\n",
    "        hierarchy_labels = agg_clustering.fit(distance_matrix).children_\n",
    "\n",
    "        # Step 4: Handle similar constraints (at higher levels)\n",
    "        for i, j in self.similar:\n",
    "            # Modify the linkage process to ensure similar nodes are grouped at higher levels\n",
    "            distance_matrix[i, j] = min(distance_matrix[i, j], 0.5)\n",
    "            distance_matrix[j, i] = min(distance_matrix[j, i], 0.5)\n",
    "\n",
    "        # Return the final hierarchical structure (children_ array in Agglomerative Clustering)\n",
    "        return hierarchy_labels\n",
    "\n",
    "# Example of calling this class with data and constraints\n",
    "def hierarchical_clustering_with_constraints(X, must_link=None, similar=None):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering with must-link and similar constraints.\n",
    "    :param X: Data matrix of shape (n_samples, n_features).\n",
    "    :param must_link: List of must-link constraints.\n",
    "    :param similar: List of similar constraints.\n",
    "    :return: Hierarchical clustering labels.\n",
    "    \"\"\"\n",
    "    clustering_model = HierarchicalClusteringWithConstraints(must_link, similar)\n",
    "    hierarchy = clustering_model.fit(X)\n",
    "    return hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae9d6af7-0aab-4fcb-b42d-e7beca14736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 7 10]\n",
      " [ 4  8]\n",
      " [ 6 12]\n",
      " [ 9 13]\n",
      " [ 5 15]\n",
      " [11 16]\n",
      " [14 17]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "X = np.random.rand(10, 5)  # Example data with 10 samples and 5 features\n",
    "must_link = [(0, 1), (2, 3)]  # Example must-link constraints\n",
    "similar = [(4, 5), (6, 7)]  # Example similar constraints\n",
    "\n",
    "hierarchy = hierarchical_clustering_with_constraints(X, must_link, similar)\n",
    "print(hierarchy)  # This will print the hierarchical clustering structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f5fcab-fa11-4092-a559-5b8f7c041097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import heapq\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "\n",
    "def get_id():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "def graph_parse(adj_matrix):\n",
    "    g_num_nodes = adj_matrix.shape[0]\n",
    "    adj_table = {}\n",
    "    VOL = 0\n",
    "    node_vol = []\n",
    "    for i in range(g_num_nodes):\n",
    "        n_v = 0\n",
    "        adj = set()\n",
    "        for j in range(g_num_nodes):\n",
    "            if adj_matrix[i,j] != 0:\n",
    "                n_v += adj_matrix[i,j]\n",
    "                VOL += adj_matrix[i,j]\n",
    "                adj.add(j)\n",
    "        adj_table[i] = adj\n",
    "        node_vol.append(n_v)\n",
    "    return g_num_nodes,VOL,node_vol,adj_table\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def cut_volume(adj_matrix,p1,p2):\n",
    "    c12 = 0\n",
    "    for i in range(len(p1)):\n",
    "        for j in range(len(p2)):\n",
    "            c = adj_matrix[p1[i],p2[j]]\n",
    "            if c != 0:\n",
    "                c12 += c\n",
    "    return c12\n",
    "\n",
    "def LayerFirst(node_dict,start_id):\n",
    "    stack = [start_id]\n",
    "    while len(stack) != 0:\n",
    "        node_id = stack.pop(0)\n",
    "        yield node_id\n",
    "        if node_dict[node_id].children:\n",
    "            for c_id in node_dict[node_id].children:\n",
    "                stack.append(c_id)\n",
    "\n",
    "def merge(new_ID, id1, id2, cut_v, cut_v_con, node_dict):\n",
    "    new_partition = node_dict[id1].partition + node_dict[id2].partition\n",
    "    v = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g = node_dict[id1].g + node_dict[id2].g - 2 * cut_v\n",
    "    v_con = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g_con = node_dict[id1].g_con + node_dict[id2].g_con - 2*cut_v_con\n",
    "    child_h = max(node_dict[id1].child_h,node_dict[id2].child_h) + 1\n",
    "    new_node = PartitionTreeNode(ID=new_ID,partition=new_partition,children={id1,id2},\n",
    "                                 g=g, vol=v, g_con=g_con, vol_con=v_con,child_h= child_h,child_cut = cut_v)\n",
    "    node_dict[id1].parent = new_ID\n",
    "    node_dict[id2].parent = new_ID\n",
    "    node_dict[new_ID] = new_node\n",
    "\n",
    "def compressNode(node_dict, node_id, parent_id):\n",
    "    p_child_h = node_dict[parent_id].child_h\n",
    "    node_children = node_dict[node_id].children\n",
    "    node_dict[parent_id].child_cut += node_dict[node_id].child_cut\n",
    "    node_dict[parent_id].children.remove(node_id)\n",
    "    node_dict[parent_id].children = node_dict[parent_id].children.union(node_children)\n",
    "    for c in node_children:\n",
    "        node_dict[c].parent = parent_id\n",
    "    com_node_child_h = node_dict[node_id].child_h\n",
    "    node_dict.pop(node_id)\n",
    "\n",
    "    if (p_child_h - com_node_child_h) == 1:\n",
    "        while True:\n",
    "            max_child_h = max([node_dict[f_c].child_h for f_c in node_dict[parent_id].children])\n",
    "            if node_dict[parent_id].child_h == (max_child_h + 1):\n",
    "                break\n",
    "            node_dict[parent_id].child_h = max_child_h + 1\n",
    "            parent_id = node_dict[parent_id].parent\n",
    "            if parent_id is None:\n",
    "                break\n",
    "\n",
    "def child_tree_deepth(node_dict,nid):\n",
    "    node = node_dict[nid]\n",
    "    deepth = 0\n",
    "    while node.parent is not None:\n",
    "        node = node_dict[node.parent]\n",
    "        deepth+=1\n",
    "    deepth += node_dict[nid].child_h\n",
    "    return deepth\n",
    "\n",
    "\n",
    "\n",
    "class PartitionTreeNode():\n",
    "    def __init__(self, ID, partition, vol, g, vol_con, g_con, children:set = None,parent = None,child_h = 0, child_cut = 0):\n",
    "        self.ID = ID\n",
    "        self.partition = partition\n",
    "        self.parent = parent\n",
    "        self.children = children\n",
    "        self.vol = vol\n",
    "        self.g = g\n",
    "        self.vol_con = vol_con\n",
    "        self.g_con = g_con\n",
    "        self.merged = False\n",
    "        self.child_h = child_h\n",
    "        self.child_cut = child_cut\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{\" + \"{}:{}\".format(self.__class__.__name__, self.gatherAttrs()) + \"}\"\n",
    "\n",
    "    def gatherAttrs(self):\n",
    "        return \",\".join(\"{}={}\"\n",
    "                        .format(k, getattr(self, k))\n",
    "                        for k in self.__dict__.keys())\n",
    "\n",
    "class PartitionTree_SSE():\n",
    "    def __init__(self,adj_matrix, adj_matrix_con, mustlink_first=False):\n",
    "        self.mustlink_first = mustlink_first\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.adj_matrix_con = adj_matrix_con\n",
    "        self.tree_node = {}\n",
    "        self.g_num_nodes, self.VOL, self.node_vol, self.adj_table = graph_parse(adj_matrix)\n",
    "        _, _, self.node_vol_con, self.adj_table_con = graph_parse(adj_matrix_con)\n",
    "        self.id_g = get_id()\n",
    "        self.leaves = []\n",
    "        self.SE = 0\n",
    "        self.SSE = 0\n",
    "        self.build_leaves()\n",
    "\n",
    "    def CombineDelta(self, node1, node2, cut_v, cut_v_con, g_vol):\n",
    "        v1 = node1.vol\n",
    "        v2 = node2.vol\n",
    "        g1 = node1.g\n",
    "        g2 = node2.g\n",
    "        v12 = v1 + v2\n",
    "        if len(node1.partition)==1:\n",
    "            cut_v_con -= node1.g_con/2\n",
    "        if len(node2.partition)==1:\n",
    "            cut_v_con -= node2.g_con/2\n",
    "        return -(2 * (cut_v+cut_v_con) / g_vol) * np.log2(g_vol / v12)\n",
    "\n",
    "    def CompressDelta(self, node1, p_node, g_vol):\n",
    "        assert node1.children is not None\n",
    "        children = node1.children\n",
    "        cut_sum = 0\n",
    "        for child in children:\n",
    "            cut_sum += self.tree_node[child].g\n",
    "            if len(self.tree_node[child].partition) == 1:\n",
    "                continue\n",
    "            cut_sum += self.tree_node[child].g_con\n",
    "        return -((cut_sum - node1.g - node1.g_con) / g_vol) * np.log2(node1.vol / p_node.vol)\n",
    "\n",
    "    def build_leaves(self):\n",
    "        for vertex in range(self.g_num_nodes):\n",
    "            ID = next(self.id_g)\n",
    "            v = self.node_vol[vertex]\n",
    "            v_con = self.node_vol_con[vertex]\n",
    "            leaf_node = PartitionTreeNode(ID=ID, partition=[vertex], g = v, vol=v, g_con = v_con, vol_con = v_con)\n",
    "            self.tree_node[ID] = leaf_node\n",
    "            self.leaves.append(ID)\n",
    "            # self.root_node.children.add(ID)\n",
    "            self.SE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "            self.SSE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "\n",
    "    def entropy(self,node_dict = None):\n",
    "        if node_dict is None:\n",
    "            node_dict = self.tree_node\n",
    "        ent = 0\n",
    "        for node_id,node in node_dict.items():\n",
    "            if node.parent is not None:\n",
    "                node_p = node_dict[node.parent]\n",
    "                node_vol = node.vol\n",
    "                node_g = node.g\n",
    "                node_p_vol = node_p.vol\n",
    "                ent += - (node_g / self.VOL) * np.log2(node_vol / node_p_vol)\n",
    "        return ent\n",
    "\n",
    "    def __build_k_tree(self, g_vol, nodes_dict:dict, k=None,):\n",
    "        min_heap = []\n",
    "        cmp_heap = []\n",
    "        nodes_ids = nodes_dict.keys()\n",
    "        new_id = None\n",
    "        for i in nodes_ids:\n",
    "            for j in self.adj_table[i]:\n",
    "                if j > i:\n",
    "                    n1 = nodes_dict[i]\n",
    "                    n2 = nodes_dict[j]\n",
    "                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n",
    "                        cut_v = self.adj_matrix[n1.partition[0],n2.partition[0]]\n",
    "                    else:\n",
    "                        cut_v = cut_volume(self.adj_matrix, p1=np.array(n1.partition), p2=np.array(n2.partition))\n",
    "                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n",
    "                        cut_v_con = self.adj_matrix_con[n1.partition[0], n2.partition[0]]\n",
    "                    else:\n",
    "                        cut_v_con = cut_volume(self.adj_matrix_con, p1=np.array(n1.partition), p2=np.array(n2.partition))\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "\n",
    "                    diff = self.CombineDelta(nodes_dict[i], nodes_dict[j], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, diff, i, j, cut_v, cut_v_con))\n",
    "        unmerged_count = len(nodes_ids)\n",
    "        while unmerged_count > 1:\n",
    "            if len(min_heap) == 0:\n",
    "                break\n",
    "            pair_mustlink, diff, id1, id2, cut_v, cut_v_con = heapq.heappop(min_heap)\n",
    "            pair_mustlink = -pair_mustlink\n",
    "            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n",
    "                continue\n",
    "            nodes_dict[id1].merged = True\n",
    "            nodes_dict[id2].merged = True\n",
    "            new_id = next(self.id_g)\n",
    "            merge(new_id, id1, id2, cut_v, cut_v_con, nodes_dict)\n",
    "            self.SE += diff\n",
    "            self.adj_table[new_id] = self.adj_table[id1].union(self.adj_table[id2])\n",
    "            for i in self.adj_table[new_id]:\n",
    "                self.adj_table[i].add(new_id)\n",
    "            #compress delta\n",
    "            if nodes_dict[id1].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id1],nodes_dict[new_id], g_vol),id1,new_id])\n",
    "            if nodes_dict[id2].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id2],nodes_dict[new_id], g_vol),id2,new_id])\n",
    "            unmerged_count -= 1\n",
    "\n",
    "            for ID in self.adj_table[new_id]:\n",
    "                if not nodes_dict[ID].merged:\n",
    "                    n1 = nodes_dict[ID]\n",
    "                    n2 = nodes_dict[new_id]\n",
    "                    cut_v = cut_volume(self.adj_matrix,np.array(n1.partition), np.array(n2.partition))\n",
    "                    cut_v_con = cut_volume(self.adj_matrix_con,np.array(n1.partition), np.array(n2.partition))\n",
    "\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "                    new_diff = self.CombineDelta(nodes_dict[ID], nodes_dict[new_id], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, new_diff, ID, new_id, cut_v, cut_v_con))\n",
    "        root = new_id\n",
    "\n",
    "        if unmerged_count > 1:\n",
    "            #combine solitary node\n",
    "            # print('processing solitary node')\n",
    "            assert len(min_heap) == 0\n",
    "            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n",
    "            new_child_h = max([nodes_dict[i].child_h for i in unmerged_nodes]) + 1\n",
    "\n",
    "            new_id = next(self.id_g)\n",
    "            new_node = PartitionTreeNode(ID=new_id,partition=list(range(self.g_num_nodes)),children=unmerged_nodes,\n",
    "                                         vol=g_vol,g = 0, vol_con=None, g_con=None ,child_h=new_child_h)\n",
    "            nodes_dict[new_id] = new_node\n",
    "\n",
    "            for i in unmerged_nodes:\n",
    "                nodes_dict[i].merged = True\n",
    "                nodes_dict[i].parent = new_id\n",
    "                if nodes_dict[i].child_h > 0:\n",
    "                    heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[i], nodes_dict[new_id], g_vol), i, new_id])\n",
    "            root = new_id\n",
    "        tree_node_copy = copy.deepcopy(self.tree_node)\n",
    "\n",
    "        if k is not None:\n",
    "            while nodes_dict[root].child_h > k:\n",
    "                diff, node_id, p_id = heapq.heappop(cmp_heap)\n",
    "                if child_tree_deepth(nodes_dict, node_id) <= k:\n",
    "                    continue\n",
    "                children = nodes_dict[node_id].children\n",
    "                compressNode(nodes_dict, node_id, p_id)\n",
    "                self.SE += diff\n",
    "                if nodes_dict[root].child_h == k:\n",
    "                    break\n",
    "                for e in cmp_heap:\n",
    "                    if e[1] == p_id:\n",
    "                        if child_tree_deepth(nodes_dict, p_id) > k:\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[e[2]], g_vol)\n",
    "                    if e[1] in children:\n",
    "                        if nodes_dict[e[1]].child_h == 0:\n",
    "                            continue\n",
    "                        if child_tree_deepth(nodes_dict, e[1]) > k:\n",
    "                            e[2] = p_id\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[p_id], g_vol)\n",
    "                heapq.heapify(cmp_heap)\n",
    "        return root, tree_node_copy\n",
    "\n",
    "\n",
    "    def build_coding_tree(self, k=2, mode='v1'):\n",
    "        if k == 1:\n",
    "            return\n",
    "        if mode == 'v1' or k is None:\n",
    "            self.root_id, hierarchical_tree_node = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n",
    "            return self.root_id, hierarchical_tree_node\n",
    "\n",
    "\n",
    "def cal_dendrogram_purity(root_id, tree_node, n_instance, y):\n",
    "    gt_list = dict()\n",
    "    for label in np.unique(y):\n",
    "        gt_list[label] = np.argwhere(y == label).flatten()\n",
    "    dp_mtx = np.zeros([n_instance,n_instance])\n",
    "    calculated_mtx = np.zeros_like(dp_mtx, dtype=bool)\n",
    "    bfs_order = []\n",
    "    bfs_queue = []\n",
    "    bfs_queue.append(root_id)\n",
    "    while len(bfs_queue) > 0:\n",
    "        nodei_id = bfs_queue.pop()\n",
    "        bfs_order.append(nodei_id)\n",
    "        nodei = tree_node[nodei_id]\n",
    "        if nodei.children is not None:\n",
    "            for child_id in nodei.children:\n",
    "                bfs_queue.append(child_id)\n",
    "    bfs_order.reverse()\n",
    "    assert len(bfs_order)==len(tree_node.keys())\n",
    "\n",
    "    for nodej_id in bfs_order:\n",
    "        commj = tree_node[nodej_id].partition\n",
    "        commj_purity = dict()\n",
    "        for gtk in gt_list.keys():\n",
    "            purity_jk = len(set(commj).intersection(set(gt_list[gtk]))) / len(set(commj))\n",
    "            commj_purity[gtk] = purity_jk\n",
    "        for m in range(len(commj)):\n",
    "            for n in range(m+1, len(commj)):\n",
    "                if (y[commj[m]] == y[commj[n]]) and (calculated_mtx[commj[m], commj[n]] == False):\n",
    "                    dp_mtx[commj[m], commj[n]] = commj_purity[y[commj[m]]]\n",
    "                    calculated_mtx[commj[m], commj[n]] = True\n",
    "    dp = np.sum(dp_mtx) / np.sum(calculated_mtx.astype(float))\n",
    "    return dp\n",
    "\n",
    "\n",
    "class PartitionTree_SSE_Constraints(PartitionTree_SSE):\n",
    "    def __init__(self, adj_matrix, adj_matrix_con, mustlink_first=False, must_link_pairs=None, cant_link_pairs=None):\n",
    "        super().__init__(adj_matrix, adj_matrix_con, mustlink_first)\n",
    "        self.must_link_pairs = must_link_pairs if must_link_pairs else []\n",
    "        self.cant_link_pairs = cant_link_pairs if cant_link_pairs else []\n",
    "        self.process_constraints()\n",
    "\n",
    "    def process_constraints(self):\n",
    "        # Initialize union-find data structure for must-link constraints\n",
    "        n = self.g_num_nodes\n",
    "        parent = [i for i in range(n)]\n",
    "\n",
    "        def find(i):\n",
    "            while parent[i] != i:\n",
    "                parent[i] = parent[parent[i]]\n",
    "                i = parent[i]\n",
    "            return i\n",
    "\n",
    "        def union(i, j):\n",
    "            pi = find(i)\n",
    "            pj = find(j)\n",
    "            if pi != pj:\n",
    "                parent[pi] = pj\n",
    "\n",
    "        # Process must-link constraints\n",
    "        for i, j in self.must_link_pairs:\n",
    "            union(i, j)\n",
    "\n",
    "        # Map from root to cluster members\n",
    "        clusters = {}\n",
    "        for i in range(n):\n",
    "            pi = find(i)\n",
    "            clusters.setdefault(pi, []).append(i)\n",
    "\n",
    "        # Build initial clusters (leaves) based on must-link clusters\n",
    "        self.leaves = []\n",
    "        self.tree_node = {}\n",
    "        self.cluster_id_map = {}\n",
    "        for idx, (root, cluster_nodes) in enumerate(clusters.items()):\n",
    "            self.cluster_id_map[root] = idx\n",
    "            ID = next(self.id_g)\n",
    "            v = sum([self.node_vol[node] for node in cluster_nodes])\n",
    "            v_con = sum([self.node_vol_con[node] for node in cluster_nodes])\n",
    "            leaf_node = PartitionTreeNode(ID=ID, partition=cluster_nodes, g=v, vol=v, g_con=v_con, vol_con=v_con)\n",
    "            self.tree_node[ID] = leaf_node\n",
    "            self.leaves.append(ID)\n",
    "            self.SE -= (v / self.VOL) * np.log2(v / self.VOL)\n",
    "            self.SSE -= (v / self.VOL) * np.log2(v / self.VOL)\n",
    "        # Build adjacency table for clusters\n",
    "        self.build_adj_table_clusters(clusters, parent, find)\n",
    "\n",
    "    def build_adj_table_clusters(self, clusters, parent, find):\n",
    "        self.adj_table_clusters = {}\n",
    "        for idx, (root, cluster_nodes) in enumerate(clusters.items()):\n",
    "            adj_clusters = set()\n",
    "            for node in cluster_nodes:\n",
    "                neighbors = self.adj_table[node]\n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_root = find(neighbor)\n",
    "                    neighbor_idx = self.cluster_id_map.get(neighbor_root)\n",
    "                    if neighbor_idx is not None and neighbor_idx != idx:\n",
    "                        adj_clusters.add(neighbor_idx)\n",
    "            self.adj_table_clusters[idx] = adj_clusters\n",
    "\n",
    "        # Build can't-link sets for clusters\n",
    "        self.cant_link_set = set()\n",
    "        for i, j in self.cant_link_pairs:\n",
    "            pi = find(i)\n",
    "            pj = find(j)\n",
    "            ci = self.cluster_id_map.get(pi)\n",
    "            cj = self.cluster_id_map.get(pj)\n",
    "            if ci is not None and cj is not None:\n",
    "                if ci != cj:\n",
    "                    self.cant_link_set.add((min(ci, cj), max(ci, cj)))\n",
    "                else:\n",
    "                    raise ValueError(\"Can't-link constraint between nodes in the same must-link cluster.\")\n",
    "\n",
    "    def violates_cant_link(self, cluster1_nodes, cluster2_nodes):\n",
    "        # Check if any pair of nodes between clusters are in can't-link constraints\n",
    "        for i in cluster1_nodes:\n",
    "            for j in cluster2_nodes:\n",
    "                if (min(i, j), max(i, j)) in self.cant_link_pairs:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __build_k_tree(self, g_vol, nodes_dict: dict, k=None):\n",
    "        min_heap = []\n",
    "        cmp_heap = []\n",
    "        nodes_ids = nodes_dict.keys()\n",
    "        adj_table = self.adj_table_clusters  # Use cluster-level adjacency table\n",
    "        new_id = None\n",
    "        for i in nodes_ids:\n",
    "            for j in adj_table.get(i, set()):\n",
    "                if j > i:\n",
    "                    n1 = nodes_dict[i]\n",
    "                    n2 = nodes_dict[j]\n",
    "                    # Compute cut_v and cut_v_con between clusters\n",
    "                    cut_v = cut_volume(self.adj_matrix, np.array(n1.partition), np.array(n2.partition))\n",
    "                    cut_v_con = cut_volume(self.adj_matrix_con, np.array(n1.partition), np.array(n2.partition))\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "\n",
    "                    diff = self.CombineDelta(nodes_dict[i], nodes_dict[j], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, diff, i, j, cut_v, cut_v_con))\n",
    "        unmerged_count = len(nodes_ids)\n",
    "        while unmerged_count > 1:\n",
    "            if len(min_heap) == 0:\n",
    "                break\n",
    "            pair_mustlink, diff, id1, id2, cut_v, cut_v_con = heapq.heappop(min_heap)\n",
    "            pair_mustlink = -pair_mustlink\n",
    "            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n",
    "                continue\n",
    "            # Before merging, check can't-link constraints\n",
    "            if (min(id1, id2), max(id1, id2)) in self.cant_link_set:\n",
    "                continue  # Cannot merge clusters due to can't-link constraint\n",
    "            nodes_dict[id1].merged = True\n",
    "            nodes_dict[id2].merged = True\n",
    "            new_id = next(self.id_g)\n",
    "            merge(new_id, id1, id2, cut_v, cut_v_con, nodes_dict)\n",
    "            self.SE += diff\n",
    "            # Update adjacency table\n",
    "            adj_table[new_id] = adj_table.get(id1, set()).union(adj_table.get(id2, set()))\n",
    "            adj_table[new_id].discard(id1)\n",
    "            adj_table[new_id].discard(id2)\n",
    "            for i in adj_table[new_id]:\n",
    "                adj_table[i].add(new_id)\n",
    "                adj_table[i].discard(id1)\n",
    "                adj_table[i].discard(id2)\n",
    "            # Remove merged nodes from adjacency table\n",
    "            adj_table.pop(id1, None)\n",
    "            adj_table.pop(id2, None)\n",
    "            # Compress delta\n",
    "            if nodes_dict[id1].child_h > 0:\n",
    "                heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[id1], nodes_dict[new_id], g_vol), id1, new_id])\n",
    "            if nodes_dict[id2].child_h > 0:\n",
    "                heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[id2], nodes_dict[new_id], g_vol), id2, new_id])\n",
    "            unmerged_count -= 1\n",
    "\n",
    "            for ID in adj_table[new_id]:\n",
    "                if not nodes_dict[ID].merged:\n",
    "                    n1 = nodes_dict[ID]\n",
    "                    n2 = nodes_dict[new_id]\n",
    "                    cut_v = cut_volume(self.adj_matrix, np.array(n1.partition), np.array(n2.partition))\n",
    "                    cut_v_con = cut_volume(self.adj_matrix_con, np.array(n1.partition), np.array(n2.partition))\n",
    "\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "                    new_diff = self.CombineDelta(nodes_dict[ID], nodes_dict[new_id], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, new_diff, ID, new_id, cut_v, cut_v_con))\n",
    "        root = new_id\n",
    "\n",
    "        if unmerged_count > 1:\n",
    "            # Combine solitary nodes\n",
    "            assert len(min_heap) == 0\n",
    "            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n",
    "            new_child_h = max([nodes_dict[i].child_h for i in unmerged_nodes]) + 1\n",
    "\n",
    "            new_id = next(self.id_g)\n",
    "            new_node = PartitionTreeNode(ID=new_id, partition=list(range(self.g_num_nodes)), children=unmerged_nodes,\n",
    "                                         vol=g_vol, g=0, vol_con=None, g_con=None, child_h=new_child_h)\n",
    "            nodes_dict[new_id] = new_node\n",
    "\n",
    "            for i in unmerged_nodes:\n",
    "                nodes_dict[i].merged = True\n",
    "                nodes_dict[i].parent = new_id\n",
    "                if nodes_dict[i].child_h > 0:\n",
    "                    heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[i], nodes_dict[new_id], g_vol), i, new_id])\n",
    "            root = new_id\n",
    "        tree_node_copy = copy.deepcopy(self.tree_node)\n",
    "\n",
    "        if k is not None:\n",
    "            while nodes_dict[root].child_h > k:\n",
    "                diff, node_id, p_id = heapq.heappop(cmp_heap)\n",
    "                if child_tree_deepth(nodes_dict, node_id) <= k:\n",
    "                    continue\n",
    "                children = nodes_dict[node_id].children\n",
    "                compressNode(nodes_dict, node_id, p_id)\n",
    "                self.SE += diff\n",
    "                if nodes_dict[root].child_h == k:\n",
    "                    break\n",
    "                for e in cmp_heap:\n",
    "                    if e[1] == p_id:\n",
    "                        if child_tree_deepth(nodes_dict, p_id) > k:\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[e[2]], g_vol)\n",
    "                    if e[1] in children:\n",
    "                        if nodes_dict[e[1]].child_h == 0:\n",
    "                            continue\n",
    "                        if child_tree_deepth(nodes_dict, e[1]) > k:\n",
    "                            e[2] = p_id\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[p_id], g_vol)\n",
    "                heapq.heapify(cmp_heap)\n",
    "        return root, tree_node_copy\n",
    "\n",
    "    def build_coding_tree(self, k=2, mode='v1'):\n",
    "        if k == 1:\n",
    "            return\n",
    "        if mode == 'v1' or k is None:\n",
    "            self.root_id, hierarchical_tree_node = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n",
    "            return self.root_id, hierarchical_tree_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d11838b-ef15-446c-8ab0-0fb7f08eba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node ID: 8\n",
      "Node ID: 0, Partition: [0], Children: None\n",
      "Node ID: 1, Partition: [1], Children: None\n",
      "Node ID: 2, Partition: [2], Children: None\n",
      "Node ID: 3, Partition: [3], Children: None\n",
      "Node ID: 4, Partition: [4], Children: None\n",
      "Node ID: 5, Partition: [1, 4], Children: {1, 4}\n",
      "Node ID: 6, Partition: [0, 1, 4], Children: {0, 5}\n",
      "Node ID: 7, Partition: [2, 0, 1, 4], Children: {2, 6}\n",
      "Node ID: 8, Partition: [3, 2, 0, 1, 4], Children: {3, 7}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    undirected_adj = np.array([\n",
    "        [0, 3, 5, 8, 0],\n",
    "        [3, 0, 6, 4, 11],\n",
    "        [5, 6, 0, 2, 0],\n",
    "        [8, 4, 2, 0, 10],\n",
    "        [0, 11, 0, 10, 0]\n",
    "    ])\n",
    "    \n",
    "    # Constrained adjacency matrix\n",
    "    adj_matrix_con = np.array([\n",
    "        [0, 1, 0, 0, 0],  # No constraints on node 0\n",
    "        [0, 0, 0, 0, 1],  # Must-link between nodes 1 and 4\n",
    "        [0, 0, 0, -1, 0], # Cannot-link between nodes 2 and 3\n",
    "        [0, 0, -1, 0, 0], # Cannot-link between nodes 2 and 3\n",
    "        [0, 1, 0, 0, 0]   # Must-link between nodes 1 and 4\n",
    "    ])\n",
    "    \n",
    "    # Initialize PartitionTree_SSE with constraints\n",
    "    tree_builder = PartitionTree_SSE(adj_matrix=undirected_adj, adj_matrix_con=adj_matrix_con, mustlink_first=True)\n",
    "    root_id, hierarchical_tree_node = tree_builder.build_coding_tree(k=2, mode='v1')\n",
    "\n",
    "    print(f\"Root Node ID: {root_id}\")\n",
    "    for node_id, node in hierarchical_tree_node.items():\n",
    "        print(f\"Node ID: {node_id}, Partition: {node.partition}, Children: {node.children}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823e0f9-6c07-40e5-b06c-4c6d096a11dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc3372-304f-4d74-a5d9-100d248cd4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "294ebc57-60db-42fd-be96-9b0041ba6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import heapq\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "\n",
    "def get_id():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "def graph_parse(adj_matrix):\n",
    "    g_num_nodes = adj_matrix.shape[0]\n",
    "    adj_table = {}\n",
    "    VOL = 0\n",
    "    node_vol = []\n",
    "    for i in range(g_num_nodes):\n",
    "        n_v = 0\n",
    "        adj = set()\n",
    "        for j in range(g_num_nodes):\n",
    "            if adj_matrix[i,j] != 0:\n",
    "                n_v += adj_matrix[i,j]\n",
    "                VOL += adj_matrix[i,j]\n",
    "                adj.add(j)\n",
    "        adj_table[i] = adj\n",
    "        node_vol.append(n_v)\n",
    "    return g_num_nodes,VOL,node_vol,adj_table\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def cut_volume(adj_matrix,p1,p2):\n",
    "    c12 = 0\n",
    "    for i in range(len(p1)):\n",
    "        for j in range(len(p2)):\n",
    "            c = adj_matrix[p1[i],p2[j]]\n",
    "            if c != 0:\n",
    "                c12 += c\n",
    "    return c12\n",
    "\n",
    "def LayerFirst(node_dict,start_id):\n",
    "    stack = [start_id]\n",
    "    while len(stack) != 0:\n",
    "        node_id = stack.pop(0)\n",
    "        yield node_id\n",
    "        if node_dict[node_id].children:\n",
    "            for c_id in node_dict[node_id].children:\n",
    "                stack.append(c_id)\n",
    "\n",
    "def merge(new_ID, id1, id2, cut_v, cut_v_con, node_dict):\n",
    "    new_partition = node_dict[id1].partition + node_dict[id2].partition\n",
    "    v = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g = node_dict[id1].g + node_dict[id2].g - 2 * cut_v\n",
    "    v_con = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g_con = node_dict[id1].g_con + node_dict[id2].g_con - 2*cut_v_con\n",
    "    child_h = max(node_dict[id1].child_h,node_dict[id2].child_h) + 1\n",
    "    new_node = PartitionTreeNode(ID=new_ID,partition=new_partition,children={id1,id2},\n",
    "                                 g=g, vol=v, g_con=g_con, vol_con=v_con,child_h= child_h,child_cut = cut_v)\n",
    "    node_dict[id1].parent = new_ID\n",
    "    node_dict[id2].parent = new_ID\n",
    "    node_dict[new_ID] = new_node\n",
    "\n",
    "def compressNode(node_dict, node_id, parent_id):\n",
    "    p_child_h = node_dict[parent_id].child_h\n",
    "    node_children = node_dict[node_id].children\n",
    "    node_dict[parent_id].child_cut += node_dict[node_id].child_cut\n",
    "    node_dict[parent_id].children.remove(node_id)\n",
    "    node_dict[parent_id].children = node_dict[parent_id].children.union(node_children)\n",
    "    for c in node_children:\n",
    "        node_dict[c].parent = parent_id\n",
    "    com_node_child_h = node_dict[node_id].child_h\n",
    "    node_dict.pop(node_id)\n",
    "\n",
    "    if (p_child_h - com_node_child_h) == 1:\n",
    "        while True:\n",
    "            max_child_h = max([node_dict[f_c].child_h for f_c in node_dict[parent_id].children])\n",
    "            if node_dict[parent_id].child_h == (max_child_h + 1):\n",
    "                break\n",
    "            node_dict[parent_id].child_h = max_child_h + 1\n",
    "            parent_id = node_dict[parent_id].parent\n",
    "            if parent_id is None:\n",
    "                break\n",
    "\n",
    "def child_tree_deepth(node_dict,nid):\n",
    "    node = node_dict[nid]\n",
    "    deepth = 0\n",
    "    while node.parent is not None:\n",
    "        node = node_dict[node.parent]\n",
    "        deepth+=1\n",
    "    deepth += node_dict[nid].child_h\n",
    "    return deepth\n",
    "\n",
    "\n",
    "\n",
    "class PartitionTreeNode():\n",
    "    def __init__(self, ID, partition, vol, g, vol_con, g_con, children:set = None,parent = None,child_h = 0, child_cut = 0):\n",
    "        self.ID = ID\n",
    "        self.partition = partition\n",
    "        self.parent = parent\n",
    "        self.children = children\n",
    "        self.vol = vol\n",
    "        self.g = g\n",
    "        self.vol_con = vol_con\n",
    "        self.g_con = g_con\n",
    "        self.merged = False\n",
    "        self.child_h = child_h\n",
    "        self.child_cut = child_cut\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{\" + \"{}:{}\".format(self.__class__.__name__, self.gatherAttrs()) + \"}\"\n",
    "\n",
    "    def gatherAttrs(self):\n",
    "        return \",\".join(\"{}={}\"\n",
    "                        .format(k, getattr(self, k))\n",
    "                        for k in self.__dict__.keys())\n",
    "\n",
    "class PartitionTree_SSE():\n",
    "    def __init__(self, adj_matrix, adj_matrix_con, must_link=None, similar=None, cannot_link=None, mustlink_first=False):\n",
    "        self.mustlink_first = mustlink_first\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.adj_matrix_con = adj_matrix_con\n",
    "        self.tree_node = {}\n",
    "        self.g_num_nodes, self.VOL, self.node_vol, self.adj_table = graph_parse(adj_matrix)\n",
    "        _, _, self.node_vol_con, self.adj_table_con = graph_parse(adj_matrix_con)\n",
    "        self.id_g = get_id()\n",
    "        self.leaves = []\n",
    "        self.SE = 0\n",
    "        self.SSE = 0\n",
    "        self.must_link = must_link if must_link is not None else []\n",
    "        self.similar = similar if similar is not None else []\n",
    "        self.cannot_link = cannot_link if cannot_link is not None else []\n",
    "        self.build_leaves()\n",
    "\n",
    "    def CombineDelta(self, node1, node2, cut_v, cut_v_con, g_vol):\n",
    "        v1 = node1.vol\n",
    "        v2 = node2.vol\n",
    "        g1 = node1.g\n",
    "        g2 = node2.g\n",
    "        v12 = v1 + v2\n",
    "        if len(node1.partition)==1:\n",
    "            cut_v_con -= node1.g_con/2\n",
    "        if len(node2.partition)==1:\n",
    "            cut_v_con -= node2.g_con/2\n",
    "        return -(2 * (cut_v+cut_v_con) / g_vol) * np.log2(g_vol / v12)\n",
    "\n",
    "    def CompressDelta(self, node1, p_node, g_vol):\n",
    "        assert node1.children is not None\n",
    "        children = node1.children\n",
    "        cut_sum = 0\n",
    "        for child in children:\n",
    "            cut_sum += self.tree_node[child].g\n",
    "            if len(self.tree_node[child].partition) == 1:\n",
    "                continue\n",
    "            cut_sum += self.tree_node[child].g_con\n",
    "        return -((cut_sum - node1.g - node1.g_con) / g_vol) * np.log2(node1.vol / p_node.vol)\n",
    "\n",
    "    def build_leaves(self):\n",
    "        # Handle must-link constraints by grouping nodes into initial clusters\n",
    "        clusters = self.find_connected_components(self.g_num_nodes, self.must_link)\n",
    "        for cluster in clusters:\n",
    "            ID = next(self.id_g)\n",
    "            partition = cluster\n",
    "            v = sum([self.node_vol[vertex] for vertex in partition])\n",
    "            v_con = sum([self.node_vol_con[vertex] for vertex in partition])\n",
    "            g = v\n",
    "            g_con = v_con\n",
    "            leaf_node = PartitionTreeNode(ID=ID, partition=partition, g=g, vol=v, g_con=g_con, vol_con=v_con)\n",
    "            self.tree_node[ID] = leaf_node\n",
    "            self.leaves.append(ID)\n",
    "            # self.root_node.children.add(ID)\n",
    "            self.SE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "            self.SSE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "\n",
    "    def find_connected_components(self, n_nodes, must_link):\n",
    "        # Build the must-link graph\n",
    "        graph = {i: [] for i in range(n_nodes)}\n",
    "        for (i, j) in must_link:\n",
    "            graph[i].append(j)\n",
    "            graph[j].append(i)\n",
    "        # Find connected components\n",
    "        visited = [False] * n_nodes\n",
    "        components = []\n",
    "        for i in range(n_nodes):\n",
    "            if not visited[i]:\n",
    "                # Do BFS\n",
    "                queue = [i]\n",
    "                visited[i] = True\n",
    "                component = [i]\n",
    "                while queue:\n",
    "                    node = queue.pop(0)\n",
    "                    for neighbor in graph[node]:\n",
    "                        if not visited[neighbor]:\n",
    "                            visited[neighbor] = True\n",
    "                            queue.append(neighbor)\n",
    "                            component.append(neighbor)\n",
    "                components.append(component)\n",
    "        return components\n",
    "\n",
    "    def entropy(self,node_dict = None):\n",
    "        if node_dict is None:\n",
    "            node_dict = self.tree_node\n",
    "        ent = 0\n",
    "        for node_id,node in node_dict.items():\n",
    "            if node.parent is not None:\n",
    "                node_p = node_dict[node.parent]\n",
    "                node_vol = node.vol\n",
    "                node_g = node.g\n",
    "                node_p_vol = node_p.vol\n",
    "                ent += - (node_g / self.VOL) * np.log2(node_vol / node_p_vol)\n",
    "        return ent\n",
    "\n",
    "    def __build_k_tree(self, g_vol, nodes_dict:dict, k=None,):\n",
    "        min_heap = []\n",
    "        cmp_heap = []\n",
    "        nodes_ids = list(nodes_dict.keys())\n",
    "        new_id = None\n",
    "        # Preprocess cannot-link constraints\n",
    "        cannot_link_set = set()\n",
    "        for (i, j) in self.cannot_link:\n",
    "            cannot_link_set.add((i, j))\n",
    "            cannot_link_set.add((j, i))\n",
    "        # Preprocess similar constraints\n",
    "        similar_set = set()\n",
    "        for (i, j) in self.similar:\n",
    "            similar_set.add((i, j))\n",
    "            similar_set.add((j, i))\n",
    "        # Build initial heap\n",
    "        for idx_i in range(len(nodes_ids)):\n",
    "            i = nodes_ids[idx_i]\n",
    "            for idx_j in range(idx_i+1, len(nodes_ids)):\n",
    "                j = nodes_ids[idx_j]\n",
    "                n1 = nodes_dict[i]\n",
    "                n2 = nodes_dict[j]\n",
    "                # Skip if merge violates cannot-link constraints\n",
    "                if self.violate_cannot_link(n1.partition, n2.partition, cannot_link_set):\n",
    "                    continue\n",
    "                # Calculate cut values\n",
    "                cut_v = cut_volume(self.adj_matrix, np.array(n1.partition), np.array(n2.partition))\n",
    "                cut_v_con = cut_volume(self.adj_matrix_con, np.array(n1.partition), np.array(n2.partition))\n",
    "                # Priority for must-link and similar constraints\n",
    "                priority = 0\n",
    "                if self.mustlink_first:\n",
    "                    if self.satisfy_must_link(n1.partition, n2.partition, self.must_link):\n",
    "                        priority = -2\n",
    "                if self.satisfy_similar(n1.partition, n2.partition, self.similar):\n",
    "                    priority = -1\n",
    "                diff = self.CombineDelta(nodes_dict[i], nodes_dict[j], cut_v, cut_v_con, g_vol)\n",
    "                heapq.heappush(min_heap, (priority, diff, i, j, cut_v, cut_v_con))\n",
    "        unmerged_count = len(nodes_ids)\n",
    "        while unmerged_count > 1:\n",
    "            if len(min_heap) == 0:\n",
    "                break\n",
    "            priority, diff, id1, id2, cut_v, cut_v_con = heapq.heappop(min_heap)\n",
    "            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n",
    "                continue\n",
    "            nodes_dict[id1].merged = True\n",
    "            nodes_dict[id2].merged = True\n",
    "            new_id = next(self.id_g)\n",
    "            merge(new_id, id1, id2, cut_v, cut_v_con, nodes_dict)\n",
    "            self.SE += diff\n",
    "            #compress delta\n",
    "            if nodes_dict[id1].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id1],nodes_dict[new_id], g_vol),id1,new_id])\n",
    "            if nodes_dict[id2].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id2],nodes_dict[new_id], g_vol),id2,new_id])\n",
    "            unmerged_count -= 1\n",
    "\n",
    "            # Update heap with new merges\n",
    "            for idx_i in range(len(nodes_ids)):\n",
    "                i = nodes_ids[idx_i]\n",
    "                if nodes_dict[i].merged or i == new_id:\n",
    "                    continue\n",
    "                n1 = nodes_dict[i]\n",
    "                n2 = nodes_dict[new_id]\n",
    "                # Skip if merge violates cannot-link constraints\n",
    "                if self.violate_cannot_link(n1.partition, n2.partition, cannot_link_set):\n",
    "                    continue\n",
    "                cut_v = cut_volume(self.adj_matrix,np.array(n1.partition), np.array(n2.partition))\n",
    "                cut_v_con = cut_volume(self.adj_matrix_con,np.array(n1.partition), np.array(n2.partition))\n",
    "                # Priority for must-link and similar constraints\n",
    "                priority = 0\n",
    "                if self.mustlink_first:\n",
    "                    if self.satisfy_must_link(n1.partition, n2.partition, self.must_link):\n",
    "                        priority = -2\n",
    "                if self.satisfy_similar(n1.partition, n2.partition, self.similar):\n",
    "                    priority = -1\n",
    "                new_diff = self.CombineDelta(nodes_dict[i], nodes_dict[new_id], cut_v, cut_v_con, g_vol)\n",
    "                heapq.heappush(min_heap, (priority, new_diff, i, new_id, cut_v, cut_v_con))\n",
    "        root = new_id\n",
    "\n",
    "        if unmerged_count > 1:\n",
    "            # Combine solitary nodes\n",
    "            assert len(min_heap) == 0\n",
    "            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n",
    "            new_child_h = max([nodes_dict[i].child_h for i in unmerged_nodes]) + 1\n",
    "\n",
    "            new_id = next(self.id_g)\n",
    "            new_node = PartitionTreeNode(ID=new_id,partition=list(range(self.g_num_nodes)),children=unmerged_nodes,\n",
    "                                         vol=g_vol,g = 0, vol_con=None, g_con=None ,child_h=new_child_h)\n",
    "            nodes_dict[new_id] = new_node\n",
    "\n",
    "            for i in unmerged_nodes:\n",
    "                nodes_dict[i].merged = True\n",
    "                nodes_dict[i].parent = new_id\n",
    "                if nodes_dict[i].child_h > 0:\n",
    "                    heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[i], nodes_dict[new_id], g_vol), i, new_id])\n",
    "            root = new_id\n",
    "        tree_node_copy = copy.deepcopy(self.tree_node)\n",
    "\n",
    "        if k is not None:\n",
    "            while nodes_dict[root].child_h > k:\n",
    "                diff, node_id, p_id = heapq.heappop(cmp_heap)\n",
    "                if child_tree_deepth(nodes_dict, node_id) <= k:\n",
    "                    continue\n",
    "                children = nodes_dict[node_id].children\n",
    "                compressNode(nodes_dict, node_id, p_id)\n",
    "                self.SE += diff\n",
    "                if nodes_dict[root].child_h == k:\n",
    "                    break\n",
    "                for e in cmp_heap:\n",
    "                    if e[1] == p_id:\n",
    "                        if child_tree_deepth(nodes_dict, p_id) > k:\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[e[2]], g_vol)\n",
    "                    if e[1] in children:\n",
    "                        if nodes_dict[e[1]].child_h == 0:\n",
    "                            continue\n",
    "                        if child_tree_deepth(nodes_dict, e[1]) > k:\n",
    "                            e[2] = p_id\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[p_id], g_vol)\n",
    "                heapq.heapify(cmp_heap)\n",
    "        return root, tree_node_copy\n",
    "\n",
    "    def violate_cannot_link(self, partition1, partition2, cannot_link_set):\n",
    "        # Check if any pair violates cannot-link constraints\n",
    "        for i in partition1:\n",
    "            for j in partition2:\n",
    "                if (i, j) in cannot_link_set:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def satisfy_must_link(self, partition1, partition2, must_link):\n",
    "        # Check if any pair satisfies must-link constraints\n",
    "        must_link_set = set(must_link)\n",
    "        for i in partition1:\n",
    "            for j in partition2:\n",
    "                if (i, j) in must_link_set or (j, i) in must_link_set:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def satisfy_similar(self, partition1, partition2, similar):\n",
    "        # Check if any pair satisfies similar constraints\n",
    "        similar_set = set(similar)\n",
    "        for i in partition1:\n",
    "            for j in partition2:\n",
    "                if (i, j) in similar_set or (j, i) in similar_set:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def build_coding_tree(self, k=2, mode='v1'):\n",
    "        if k == 1:\n",
    "            return\n",
    "        if mode == 'v1' or k is None:\n",
    "            self.root_id, hierarchical_tree_node = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n",
    "            return self.root_id, hierarchical_tree_node\n",
    "\n",
    "def cal_dendrogram_purity(root_id, tree_node, n_instance, y):\n",
    "    gt_list = dict()\n",
    "    for label in np.unique(y):\n",
    "        gt_list[label] = np.argwhere(y == label).flatten()\n",
    "    dp_mtx = np.zeros([n_instance,n_instance])\n",
    "    calculated_mtx = np.zeros_like(dp_mtx, dtype=bool)\n",
    "    bfs_order = []\n",
    "    bfs_queue = []\n",
    "    bfs_queue.append(root_id)\n",
    "    while len(bfs_queue) > 0:\n",
    "        nodei_id = bfs_queue.pop()\n",
    "        bfs_order.append(nodei_id)\n",
    "        nodei = tree_node[nodei_id]\n",
    "        if nodei.children is not None:\n",
    "            for child_id in nodei.children:\n",
    "                bfs_queue.append(child_id)\n",
    "    bfs_order.reverse()\n",
    "    assert len(bfs_order)==len(tree_node.keys())\n",
    "\n",
    "    for nodej_id in bfs_order:\n",
    "        commj = tree_node[nodej_id].partition\n",
    "        commj_purity = dict()\n",
    "        for gtk in gt_list.keys():\n",
    "            purity_jk = len(set(commj).intersection(set(gt_list[gtk]))) / len(set(commj))\n",
    "            commj_purity[gtk] = purity_jk\n",
    "        for m in range(len(commj)):\n",
    "            for n in range(m+1, len(commj)):\n",
    "                if (y[commj[m]] == y[commj[n]]) and (calculated_mtx[commj[m], commj[n]] == False):\n",
    "                    dp_mtx[commj[m], commj[n]] = commj_purity[y[commj[m]]]\n",
    "                    calculated_mtx[commj[m], commj[n]] = True\n",
    "    dp = np.sum(dp_mtx) / np.sum(calculated_mtx.astype(float))\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "672d01ec-e5af-483f-af8a-212d841e169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting Clusters:\n",
      "Cluster 0: [0, 1]\n",
      "Cluster 1: [2]\n",
      "Cluster 2: [3]\n",
      "Cluster 3: [4, 5]\n",
      "\n",
      "Hierarchical Tree Structure:\n",
      "Node 6: [0, 1, 2, 3, 4, 5]\n",
      "  Node 3: [4, 5]\n",
      "  Node 5: [0, 1, 2, 3]\n",
      "    Node 0: [0, 1]\n",
      "    Node 4: [2, 3]\n",
      "      Node 1: [2]\n",
      "      Node 2: [3]\n"
     ]
    }
   ],
   "source": [
    "undirected_adj = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 1, 0],\n",
    "    [0, 1, 0, 0, 1, 1],\n",
    "    [0, 0, 1, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0]\n",
    "])\n",
    "\n",
    "# Create a secondary adjacency matrix for constraints (can be same as the main one)\n",
    "adj_matrix_con = undirected_adj.copy()\n",
    "\n",
    "# Define constraints\n",
    "must_link = [(0, 1), (4, 5)]        # Nodes 0 and 1 must be in the same leaf node; Nodes 4 and 5 must be in the same leaf node\n",
    "similar = [(2, 3)]                  # Nodes 2 and 3 should be clustered somewhere in the tree\n",
    "cannot_link = [(1, 4)]              # Nodes 1 and 4 cannot be linked\n",
    "\n",
    "# Create an instance of the PartitionTree_SSE with constraints\n",
    "tree_builder = PartitionTree_SSE(\n",
    "    adj_matrix=undirected_adj,\n",
    "    adj_matrix_con=adj_matrix_con,\n",
    "    must_link=must_link,\n",
    "    similar=similar,\n",
    "    cannot_link=cannot_link,\n",
    "    mustlink_first=True\n",
    ")\n",
    "\n",
    "# Build the coding tree\n",
    "root_id, hierarchical_tree_node = tree_builder.build_coding_tree(k=None, mode='v1')\n",
    "\n",
    "# Print the resulting clusters\n",
    "print(\"Resulting Clusters:\")\n",
    "for node_id, node in hierarchical_tree_node.items():\n",
    "    if node.children is None:  # Leaf nodes\n",
    "        print(f\"Cluster {node_id}: {node.partition}\")\n",
    "\n",
    "# Print the tree structure\n",
    "print(\"\\nHierarchical Tree Structure:\")\n",
    "def print_tree(node_id, tree_node, level=0):\n",
    "    node = tree_node[node_id]\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Node {node_id}: {node.partition}\")\n",
    "    if node.children:\n",
    "        for child_id in node.children:\n",
    "            print_tree(child_id, tree_node, level+1)\n",
    "print_tree(root_id, hierarchical_tree_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f5eeb87-7e85-4417-89e6-bd6f7d1b70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resulting Clusters:\n",
      "Cluster 0: [0, 1, 2]\n",
      "Cluster 1: [3, 4, 5]\n",
      "Cluster 2: [6, 7, 8]\n",
      "Cluster 3: [9]\n",
      "Cluster 4: [10]\n",
      "Cluster 5: [11]\n",
      "Cluster 6: [12]\n",
      "Cluster 7: [13]\n",
      "Cluster 8: [14]\n",
      "\n",
      "Hierarchical Tree Structure:\n",
      "Node 15: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "  Node 13: [13, 3, 4, 5, 6, 7, 8]\n",
      "    Node 9: [3, 4, 5, 6, 7, 8]\n",
      "      Node 1: [3, 4, 5]\n",
      "      Node 2: [6, 7, 8]\n",
      "    Node 7: [13]\n",
      "  Node 6: [12]\n",
      "  Node 14: [14, 0, 1, 2, 9, 10, 11]\n",
      "    Node 8: [14]\n",
      "    Node 12: [0, 1, 2, 9, 10, 11]\n",
      "      Node 0: [0, 1, 2]\n",
      "      Node 11: [9, 10, 11]\n",
      "        Node 10: [10, 11]\n",
      "          Node 4: [10]\n",
      "          Node 5: [11]\n",
      "        Node 3: [9]\n"
     ]
    }
   ],
   "source": [
    "# The code from the previous implementation should be placed here or imported if saved in a module.\n",
    "# For the purpose of this example, we assume the code is already available in the environment.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    # np.random.seed(42)\n",
    "\n",
    "    # Number of nodes\n",
    "    num_nodes = 15\n",
    "\n",
    "    # Generate a random adjacency matrix with weights between 1 and 10\n",
    "    adjacency_matrix = np.random.randint(0, 11, size=(num_nodes, num_nodes))\n",
    "    adjacency_matrix = np.triu(adjacency_matrix, 1)  # Upper triangle to avoid self-loops and duplicate edges\n",
    "    adjacency_matrix += adjacency_matrix.T  # Make it symmetric to represent an undirected graph\n",
    "\n",
    "    # Zero out some edges to introduce sparsity\n",
    "    mask = np.random.choice([0, 1], size=adjacency_matrix.shape, p=[0.3, 0.7])\n",
    "    adjacency_matrix = adjacency_matrix * mask\n",
    "\n",
    "    # Ensure the diagonal is zero (no self-loops)\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "    # Create a secondary adjacency matrix for constraints (can be the same as the main one)\n",
    "    adj_matrix_con = adjacency_matrix.copy()\n",
    "\n",
    "    # Define constraints\n",
    "    must_link = [\n",
    "        (0, 1), (0, 2),  # Group 1\n",
    "        (3, 4), (4, 5), (3, 5),  # Group 2\n",
    "        (6, 7), (7, 8), (6, 8),  # Group 3\n",
    "    ]\n",
    "\n",
    "    similar = [\n",
    "        (2, 3),  # Node 2 is similar to node 3\n",
    "        (5, 6),  # Node 5 is similar to node 6\n",
    "        (9, 10), (10, 11),  # Nodes 9, 10, 11 are similar\n",
    "    ]\n",
    "\n",
    "    cannot_link = [\n",
    "        (1, 4),  # Node 1 cannot be linked with node 4\n",
    "        (2, 5),  # Node 2 cannot be linked with node 5\n",
    "        (7, 9),  # Node 7 cannot be linked with node 9\n",
    "        (12, 13), (13, 14), (12, 14),  # Nodes 12, 13, 14 cannot be linked\n",
    "    ]\n",
    "\n",
    "    # Ground truth labels\n",
    "    y = np.array([\n",
    "        0, 0, 0,    # Nodes 0, 1, 2 (Group 1)\n",
    "        1, 1, 1,    # Nodes 3, 4, 5 (Group 2)\n",
    "        2, 2, 2,    # Nodes 6, 7, 8 (Group 3)\n",
    "        3, 3, 3,    # Nodes 9, 10, 11 (Group 4)\n",
    "        4, 4, 4     # Nodes 12, 13, 14 (Group 5)\n",
    "    ])\n",
    "\n",
    "    # Create an instance of the PartitionTree_SSE with constraints\n",
    "    tree_builder = PartitionTree_SSE(\n",
    "        adj_matrix=adjacency_matrix,\n",
    "        adj_matrix_con=adj_matrix_con,\n",
    "        must_link=must_link,\n",
    "        similar=similar,\n",
    "        cannot_link=cannot_link,\n",
    "        mustlink_first=True\n",
    "    )\n",
    "\n",
    "    # Build the coding tree\n",
    "    root_id, hierarchical_tree_node = tree_builder.build_coding_tree(k=5, mode='v1')\n",
    "\n",
    "    # Print the resulting clusters\n",
    "    print(\"\\nResulting Clusters:\")\n",
    "    for node_id, node in hierarchical_tree_node.items():\n",
    "        if node.children is None:  # Leaf nodes\n",
    "            print(f\"Cluster {node_id}: {node.partition}\")\n",
    "\n",
    "    # Print the tree structure\n",
    "    print(\"\\nHierarchical Tree Structure:\")\n",
    "    def print_tree(node_id, tree_node, level=0):\n",
    "        node = tree_node[node_id]\n",
    "        indent = \"  \" * level\n",
    "        print(f\"{indent}Node {node_id}: {node.partition}\")\n",
    "        if node.children:\n",
    "            for child_id in node.children:\n",
    "                print_tree(child_id, tree_node, level+1)\n",
    "    print_tree(root_id, hierarchical_tree_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2de9f-77fe-434b-ac05-b43656d11860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0e80c1f-379e-4db5-84b5-57cf7ee89018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import heapq\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "\n",
    "def get_id():\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "def graph_parse(adj_matrix):\n",
    "    g_num_nodes = adj_matrix.shape[0]\n",
    "    adj_table = {}\n",
    "    VOL = 0\n",
    "    node_vol = []\n",
    "    for i in range(g_num_nodes):\n",
    "        n_v = 0\n",
    "        adj = set()\n",
    "        for j in range(g_num_nodes):\n",
    "            if adj_matrix[i,j] != 0:\n",
    "                n_v += adj_matrix[i,j]\n",
    "                VOL += adj_matrix[i,j]\n",
    "                adj.add(j)\n",
    "        adj_table[i] = adj\n",
    "        node_vol.append(n_v)\n",
    "    return g_num_nodes,VOL,node_vol,adj_table\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def cut_volume(adj_matrix,p1,p2):\n",
    "    c12 = 0\n",
    "    for i in range(len(p1)):\n",
    "        for j in range(len(p2)):\n",
    "            c = adj_matrix[p1[i],p2[j]]\n",
    "            if c != 0:\n",
    "                c12 += c\n",
    "    return c12\n",
    "\n",
    "def LayerFirst(node_dict,start_id):\n",
    "    stack = [start_id]\n",
    "    while len(stack) != 0:\n",
    "        node_id = stack.pop(0)\n",
    "        yield node_id\n",
    "        if node_dict[node_id].children:\n",
    "            for c_id in node_dict[node_id].children:\n",
    "                stack.append(c_id)\n",
    "\n",
    "def merge(new_ID, id1, id2, cut_v, cut_v_con, node_dict):\n",
    "    new_partition = node_dict[id1].partition + node_dict[id2].partition\n",
    "    v = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g = node_dict[id1].g + node_dict[id2].g - 2 * cut_v\n",
    "    v_con = node_dict[id1].vol + node_dict[id2].vol\n",
    "    g_con = node_dict[id1].g_con + node_dict[id2].g_con - 2*cut_v_con\n",
    "    child_h = max(node_dict[id1].child_h,node_dict[id2].child_h) + 1\n",
    "    new_node = PartitionTreeNode(ID=new_ID,partition=new_partition,children={id1,id2},\n",
    "                                 g=g, vol=v, g_con=g_con, vol_con=v_con,child_h= child_h,child_cut = cut_v)\n",
    "    node_dict[id1].parent = new_ID\n",
    "    node_dict[id2].parent = new_ID\n",
    "    node_dict[new_ID] = new_node\n",
    "\n",
    "def compressNode(node_dict, node_id, parent_id):\n",
    "    p_child_h = node_dict[parent_id].child_h\n",
    "    node_children = node_dict[node_id].children\n",
    "    node_dict[parent_id].child_cut += node_dict[node_id].child_cut\n",
    "    node_dict[parent_id].children.remove(node_id)\n",
    "    node_dict[parent_id].children = node_dict[parent_id].children.union(node_children)\n",
    "    for c in node_children:\n",
    "        node_dict[c].parent = parent_id\n",
    "    com_node_child_h = node_dict[node_id].child_h\n",
    "    node_dict.pop(node_id)\n",
    "\n",
    "    if (p_child_h - com_node_child_h) == 1:\n",
    "        while True:\n",
    "            max_child_h = max([node_dict[f_c].child_h for f_c in node_dict[parent_id].children])\n",
    "            if node_dict[parent_id].child_h == (max_child_h + 1):\n",
    "                break\n",
    "            node_dict[parent_id].child_h = max_child_h + 1\n",
    "            parent_id = node_dict[parent_id].parent\n",
    "            if parent_id is None:\n",
    "                break\n",
    "\n",
    "def child_tree_deepth(node_dict,nid):\n",
    "    node = node_dict[nid]\n",
    "    deepth = 0\n",
    "    while node.parent is not None:\n",
    "        node = node_dict[node.parent]\n",
    "        deepth+=1\n",
    "    deepth += node_dict[nid].child_h\n",
    "    return deepth\n",
    "\n",
    "\n",
    "\n",
    "class PartitionTreeNode():\n",
    "    def __init__(self, ID, partition, vol, g, vol_con, g_con, children:set = None,parent = None,child_h = 0, child_cut = 0):\n",
    "        self.ID = ID\n",
    "        self.partition = partition\n",
    "        self.parent = parent\n",
    "        self.children = children\n",
    "        self.vol = vol\n",
    "        self.g = g\n",
    "        self.vol_con = vol_con\n",
    "        self.g_con = g_con\n",
    "        self.merged = False\n",
    "        self.child_h = child_h\n",
    "        self.child_cut = child_cut\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{\" + \"{}:{}\".format(self.__class__.__name__, self.gatherAttrs()) + \"}\"\n",
    "\n",
    "    def gatherAttrs(self):\n",
    "        return \",\".join(\"{}={}\"\n",
    "                        .format(k, getattr(self, k))\n",
    "                        for k in self.__dict__.keys())\n",
    "\n",
    "class PartitionTree_SSE():\n",
    "    def __init__(self,adj_matrix, adj_matrix_con, mustlink_first=False):\n",
    "        self.mustlink_first = mustlink_first\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.adj_matrix_con = adj_matrix_con\n",
    "        self.tree_node = {}\n",
    "        self.g_num_nodes, self.VOL, self.node_vol, self.adj_table = graph_parse(adj_matrix)\n",
    "        _, _, self.node_vol_con, self.adj_table_con = graph_parse(adj_matrix_con)\n",
    "        self.id_g = get_id()\n",
    "        self.leaves = []\n",
    "        self.SE = 0\n",
    "        self.SSE = 0\n",
    "        self.build_leaves()\n",
    "\n",
    "    def CombineDelta(self, node1, node2, cut_v, cut_v_con, g_vol):\n",
    "        v1 = node1.vol\n",
    "        v2 = node2.vol\n",
    "        g1 = node1.g\n",
    "        g2 = node2.g\n",
    "        v12 = v1 + v2\n",
    "        if len(node1.partition)==1:\n",
    "            cut_v_con -= node1.g_con/2\n",
    "        if len(node2.partition)==1:\n",
    "            cut_v_con -= node2.g_con/2\n",
    "        return -(2 * (cut_v+cut_v_con) / g_vol) * np.log2(g_vol / v12)\n",
    "\n",
    "    def CompressDelta(self, node1, p_node, g_vol):\n",
    "        assert node1.children is not None\n",
    "        children = node1.children\n",
    "        cut_sum = 0\n",
    "        for child in children:\n",
    "            cut_sum += self.tree_node[child].g\n",
    "            if len(self.tree_node[child].partition) == 1:\n",
    "                continue\n",
    "            cut_sum += self.tree_node[child].g_con\n",
    "        return -((cut_sum - node1.g - node1.g_con) / g_vol) * np.log2(node1.vol / p_node.vol)\n",
    "\n",
    "    def build_leaves(self):\n",
    "        for vertex in range(self.g_num_nodes):\n",
    "            ID = next(self.id_g)\n",
    "            v = self.node_vol[vertex]\n",
    "            v_con = self.node_vol_con[vertex]\n",
    "            leaf_node = PartitionTreeNode(ID=ID, partition=[vertex], g = v, vol=v, g_con = v_con, vol_con = v_con)\n",
    "            self.tree_node[ID] = leaf_node\n",
    "            self.leaves.append(ID)\n",
    "            # self.root_node.children.add(ID)\n",
    "            self.SE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "            self.SSE -= (v/self.VOL) * np.log2(v/self.VOL)\n",
    "\n",
    "    def entropy(self,node_dict = None):\n",
    "        if node_dict is None:\n",
    "            node_dict = self.tree_node\n",
    "        ent = 0\n",
    "        for node_id,node in node_dict.items():\n",
    "            if node.parent is not None:\n",
    "                node_p = node_dict[node.parent]\n",
    "                node_vol = node.vol\n",
    "                node_g = node.g\n",
    "                node_p_vol = node_p.vol\n",
    "                ent += - (node_g / self.VOL) * np.log2(node_vol / node_p_vol)\n",
    "        return ent\n",
    "\n",
    "    def __build_k_tree(self, g_vol, nodes_dict:dict, k=None,):\n",
    "        min_heap = []\n",
    "        cmp_heap = []\n",
    "        nodes_ids = nodes_dict.keys()\n",
    "        new_id = None\n",
    "        for i in nodes_ids:\n",
    "            for j in self.adj_table[i]:\n",
    "                if j > i:\n",
    "                    n1 = nodes_dict[i]\n",
    "                    n2 = nodes_dict[j]\n",
    "                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n",
    "                        cut_v = self.adj_matrix[n1.partition[0],n2.partition[0]]\n",
    "                    else:\n",
    "                        cut_v = cut_volume(self.adj_matrix, p1=np.array(n1.partition), p2=np.array(n2.partition))\n",
    "                    if len(n1.partition) == 1 and len(n2.partition) == 1:\n",
    "                        cut_v_con = self.adj_matrix_con[n1.partition[0], n2.partition[0]]\n",
    "                    else:\n",
    "                        cut_v_con = cut_volume(self.adj_matrix_con, p1=np.array(n1.partition), p2=np.array(n2.partition))\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "\n",
    "                    diff = self.CombineDelta(nodes_dict[i], nodes_dict[j], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, diff, i, j, cut_v, cut_v_con))\n",
    "        unmerged_count = len(nodes_ids)\n",
    "        while unmerged_count > 1:\n",
    "            if len(min_heap) == 0:\n",
    "                break\n",
    "            pair_mustlink, diff, id1, id2, cut_v, cut_v_con = heapq.heappop(min_heap)\n",
    "            pair_mustlink = -pair_mustlink\n",
    "            if nodes_dict[id1].merged or nodes_dict[id2].merged:\n",
    "                continue\n",
    "            nodes_dict[id1].merged = True\n",
    "            nodes_dict[id2].merged = True\n",
    "            new_id = next(self.id_g)\n",
    "            merge(new_id, id1, id2, cut_v, cut_v_con, nodes_dict)\n",
    "            self.SE += diff\n",
    "            self.adj_table[new_id] = self.adj_table[id1].union(self.adj_table[id2])\n",
    "            for i in self.adj_table[new_id]:\n",
    "                self.adj_table[i].add(new_id)\n",
    "            #compress delta\n",
    "            if nodes_dict[id1].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id1],nodes_dict[new_id], g_vol),id1,new_id])\n",
    "            if nodes_dict[id2].child_h > 0:\n",
    "                heapq.heappush(cmp_heap,[self.CompressDelta(nodes_dict[id2],nodes_dict[new_id], g_vol),id2,new_id])\n",
    "            unmerged_count -= 1\n",
    "\n",
    "            for ID in self.adj_table[new_id]:\n",
    "                if not nodes_dict[ID].merged:\n",
    "                    n1 = nodes_dict[ID]\n",
    "                    n2 = nodes_dict[new_id]\n",
    "                    cut_v = cut_volume(self.adj_matrix,np.array(n1.partition), np.array(n2.partition))\n",
    "                    cut_v_con = cut_volume(self.adj_matrix_con,np.array(n1.partition), np.array(n2.partition))\n",
    "\n",
    "                    pair_mustlink = 0\n",
    "                    if self.mustlink_first:\n",
    "                        if cut_v_con > 0:\n",
    "                            pair_mustlink = 1\n",
    "                        elif cut_v_con < 0:\n",
    "                            pair_mustlink = -1\n",
    "                    new_diff = self.CombineDelta(nodes_dict[ID], nodes_dict[new_id], cut_v, cut_v_con, g_vol)\n",
    "                    heapq.heappush(min_heap, (-pair_mustlink, new_diff, ID, new_id, cut_v, cut_v_con))\n",
    "        root = new_id\n",
    "\n",
    "        if unmerged_count > 1:\n",
    "            #combine solitary node\n",
    "            # print('processing solitary node')\n",
    "            assert len(min_heap) == 0\n",
    "            unmerged_nodes = {i for i, j in nodes_dict.items() if not j.merged}\n",
    "            new_child_h = max([nodes_dict[i].child_h for i in unmerged_nodes]) + 1\n",
    "\n",
    "            new_id = next(self.id_g)\n",
    "            new_node = PartitionTreeNode(ID=new_id,partition=list(range(self.g_num_nodes)),children=unmerged_nodes,\n",
    "                                         vol=g_vol,g = 0, vol_con=None, g_con=None ,child_h=new_child_h)\n",
    "            nodes_dict[new_id] = new_node\n",
    "\n",
    "            for i in unmerged_nodes:\n",
    "                nodes_dict[i].merged = True\n",
    "                nodes_dict[i].parent = new_id\n",
    "                if nodes_dict[i].child_h > 0:\n",
    "                    heapq.heappush(cmp_heap, [self.CompressDelta(nodes_dict[i], nodes_dict[new_id], g_vol), i, new_id])\n",
    "            root = new_id\n",
    "        tree_node_copy = copy.deepcopy(self.tree_node)\n",
    "\n",
    "        if k is not None:\n",
    "            while nodes_dict[root].child_h > k:\n",
    "                diff, node_id, p_id = heapq.heappop(cmp_heap)\n",
    "                if child_tree_deepth(nodes_dict, node_id) <= k:\n",
    "                    continue\n",
    "                children = nodes_dict[node_id].children\n",
    "                compressNode(nodes_dict, node_id, p_id)\n",
    "                self.SE += diff\n",
    "                if nodes_dict[root].child_h == k:\n",
    "                    break\n",
    "                for e in cmp_heap:\n",
    "                    if e[1] == p_id:\n",
    "                        if child_tree_deepth(nodes_dict, p_id) > k:\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[e[2]], g_vol)\n",
    "                    if e[1] in children:\n",
    "                        if nodes_dict[e[1]].child_h == 0:\n",
    "                            continue\n",
    "                        if child_tree_deepth(nodes_dict, e[1]) > k:\n",
    "                            e[2] = p_id\n",
    "                            e[0] = self.CompressDelta(nodes_dict[e[1]], nodes_dict[p_id], g_vol)\n",
    "                heapq.heapify(cmp_heap)\n",
    "        return root, tree_node_copy\n",
    "\n",
    "\n",
    "    def build_coding_tree(self, k=2, mode='v1'):\n",
    "        if k == 1:\n",
    "            return\n",
    "        if mode == 'v1' or k is None:\n",
    "            self.root_id, hierarchical_tree_node = self.__build_k_tree(self.VOL, self.tree_node, k=k)\n",
    "            return self.root_id, hierarchical_tree_node\n",
    "\n",
    "def cal_dendrogram_purity(root_id, tree_node, n_instance, y):\n",
    "    gt_list = dict()\n",
    "    for label in np.unique(y):\n",
    "        gt_list[label] = np.argwhere(y == label).flatten()\n",
    "    dp_mtx = np.zeros([n_instance,n_instance])\n",
    "    calculated_mtx = np.zeros_like(dp_mtx, dtype=bool)\n",
    "    bfs_order = []\n",
    "    bfs_queue = []\n",
    "    bfs_queue.append(root_id)\n",
    "    while len(bfs_queue) > 0:\n",
    "        nodei_id = bfs_queue.pop()\n",
    "        bfs_order.append(nodei_id)\n",
    "        nodei = tree_node[nodei_id]\n",
    "        if nodei.children is not None:\n",
    "            for child_id in nodei.children:\n",
    "                bfs_queue.append(child_id)\n",
    "    bfs_order.reverse()\n",
    "    assert len(bfs_order)==len(tree_node.keys())\n",
    "\n",
    "    for nodej_id in bfs_order:\n",
    "        commj = tree_node[nodej_id].partition\n",
    "        commj_purity = dict()\n",
    "        for gtk in gt_list.keys():\n",
    "            purity_jk = len(set(commj).intersection(set(gt_list[gtk]))) / len(set(commj))\n",
    "            commj_purity[gtk] = purity_jk\n",
    "        for m in range(len(commj)):\n",
    "            for n in range(m+1, len(commj)):\n",
    "                if (y[commj[m]] == y[commj[n]]) and (calculated_mtx[commj[m], commj[n]] == False):\n",
    "                    dp_mtx[commj[m], commj[n]] = commj_purity[y[commj[m]]]\n",
    "                    calculated_mtx[commj[m], commj[n]] = True\n",
    "    dp = np.sum(dp_mtx) / np.sum(calculated_mtx.astype(float))\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eefef3e5-d00c-42dc-94f5-e2ae022784e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree_ascii(node_id, tree_nodes, prefix='', is_last=True):\n",
    "    \"\"\"\n",
    "    Recursively prints the tree structure using ASCII characters.\n",
    "    \"\"\"\n",
    "    node = tree_nodes[node_id]\n",
    "    # Print the current node with appropriate prefix and connector\n",
    "    connector = ' ' if is_last else ' '\n",
    "    print(prefix + connector + f\"Node ID: {node.ID}, Partition: {node.partition}\")\n",
    "\n",
    "    # Prepare the prefix for child nodes\n",
    "    if node.children:\n",
    "        child_ids = list(node.children)\n",
    "        child_count = len(child_ids)\n",
    "        new_prefix = prefix + ('    ' if is_last else '   ')\n",
    "        for i, child_id in enumerate(child_ids):\n",
    "            is_last_child = (i == (child_count - 1))\n",
    "            print_tree_ascii(child_id, tree_nodes, prefix=new_prefix, is_last=is_last_child)\n",
    "\n",
    "if True:\n",
    "    import numpy as np\n",
    "\n",
    "    # Parameters for the test graph\n",
    "    N = 100  # Number of nodes in the graph (reduced for demonstration)\n",
    "    p = 0.2  # Probability of edge creation between nodes\n",
    "\n",
    "    # Set a random seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Generate indices for the upper triangular part of the adjacency matrix\n",
    "    triu_indices = np.triu_indices(N, k=1)\n",
    "\n",
    "    # Determine the number of edges based on the probability p\n",
    "    num_possible_edges = len(triu_indices[0])\n",
    "    num_edges = int(p * num_possible_edges)\n",
    "\n",
    "    # Randomly select which edges will exist\n",
    "    edge_indices = np.random.choice(num_possible_edges, size=num_edges, replace=False)\n",
    "\n",
    "    # Initialize the adjacency matrix with zeros\n",
    "    adj_matrix = np.zeros((N, N), dtype=int)\n",
    "\n",
    "    # Assign random weights between 1 and 10 to the selected edges\n",
    "    weights = np.random.randint(1, 11, size=num_edges)\n",
    "    adj_matrix[triu_indices[0][edge_indices], triu_indices[1][edge_indices]] = weights\n",
    "\n",
    "    # Make the adjacency matrix symmetric to represent an undirected graph\n",
    "    adj_matrix = adj_matrix + adj_matrix.T\n",
    "\n",
    "    # Create an adjacency matrix for constraints (all zeros in this case)\n",
    "    adj_matrix_con = np.zeros((N, N), dtype=int)\n",
    "\n",
    "    # Instantiate the PartitionTree_SSE class with the generated adjacency matrices\n",
    "    y = PartitionTree_SSE(adj_matrix=adj_matrix, adj_matrix_con=adj_matrix_con)\n",
    "\n",
    "    # Build the coding tree\n",
    "    root_id, tree_nodes = y.build_coding_tree(None, mode='v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8751482f-087a-44b6-92f0-cfd8bbc04920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Tree Structure:\n",
      " Node ID: 198, Partition: [50, 44, 26, 2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83, 1, 62, 6, 94, 19, 82, 54, 73, 59, 90, 32, 18, 33, 76, 91]\n",
      "     Node ID: 50, Partition: [50]\n",
      "     Node ID: 197, Partition: [44, 26, 2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83, 1, 62, 6, 94, 19, 82, 54, 73, 59, 90, 32, 18, 33, 76, 91]\n",
      "         Node ID: 196, Partition: [26, 2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83, 1, 62, 6, 94, 19, 82, 54, 73, 59, 90, 32, 18, 33, 76, 91]\n",
      "            Node ID: 26, Partition: [26]\n",
      "            Node ID: 195, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83, 1, 62, 6, 94, 19, 82, 54, 73, 59, 90, 32, 18, 33, 76, 91]\n",
      "                Node ID: 186, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83, 1, 62, 6, 94, 19, 82]\n",
      "                   Node ID: 185, Partition: [1, 62, 6, 94, 19, 82]\n",
      "                      Node ID: 184, Partition: [6, 94, 19, 82]\n",
      "                         Node ID: 182, Partition: [6, 94]\n",
      "                            Node ID: 94, Partition: [94]\n",
      "                            Node ID: 6, Partition: [6]\n",
      "                         Node ID: 183, Partition: [19, 82]\n",
      "                             Node ID: 82, Partition: [82]\n",
      "                             Node ID: 19, Partition: [19]\n",
      "                      Node ID: 178, Partition: [1, 62]\n",
      "                          Node ID: 1, Partition: [1]\n",
      "                          Node ID: 62, Partition: [62]\n",
      "                   Node ID: 181, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61, 51, 64, 83]\n",
      "                       Node ID: 177, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74, 34, 17, 61]\n",
      "                          Node ID: 176, Partition: [34, 17, 61]\n",
      "                             Node ID: 34, Partition: [34]\n",
      "                             Node ID: 175, Partition: [17, 61]\n",
      "                                 Node ID: 17, Partition: [17]\n",
      "                                 Node ID: 61, Partition: [61]\n",
      "                          Node ID: 174, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88, 99, 8, 74]\n",
      "                              Node ID: 171, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84, 9, 22, 88]\n",
      "                                 Node ID: 168, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80, 14, 46, 84]\n",
      "                                    Node ID: 165, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77, 37, 80]\n",
      "                                       Node ID: 163, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12, 13, 77]\n",
      "                                          Node ID: 161, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92, 3, 12]\n",
      "                                             Node ID: 160, Partition: [3, 12]\n",
      "                                                Node ID: 3, Partition: [3]\n",
      "                                                Node ID: 12, Partition: [12]\n",
      "                                             Node ID: 159, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72, 97, 41, 92]\n",
      "                                                 Node ID: 156, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81, 48, 95, 68, 72]\n",
      "                                                    Node ID: 152, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36, 57, 56, 81]\n",
      "                                                       Node ID: 149, Partition: [2, 69, 96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36]\n",
      "                                                          Node ID: 148, Partition: [96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66, 85, 24, 36]\n",
      "                                                             Node ID: 145, Partition: [96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30, 38, 66]\n",
      "                                                                Node ID: 144, Partition: [38, 66]\n",
      "                                                                   Node ID: 66, Partition: [66]\n",
      "                                                                   Node ID: 38, Partition: [38]\n",
      "                                                                Node ID: 142, Partition: [96, 25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30]\n",
      "                                                                    Node ID: 96, Partition: [96]\n",
      "                                                                    Node ID: 141, Partition: [25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65, 4, 30]\n",
      "                                                                        Node ID: 139, Partition: [25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40, 60, 65]\n",
      "                                                                           Node ID: 137, Partition: [25, 20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                              Node ID: 136, Partition: [20, 21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                 Node ID: 20, Partition: [20]\n",
      "                                                                                 Node ID: 135, Partition: [21, 28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                     Node ID: 21, Partition: [21]\n",
      "                                                                                     Node ID: 134, Partition: [28, 5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                         Node ID: 28, Partition: [28]\n",
      "                                                                                         Node ID: 133, Partition: [5, 58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                             Node ID: 132, Partition: [58, 23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                Node ID: 58, Partition: [58]\n",
      "                                                                                                Node ID: 131, Partition: [23, 0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                    Node ID: 130, Partition: [0, 10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                       Node ID: 0, Partition: [0]\n",
      "                                                                                                       Node ID: 129, Partition: [10, 43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                           Node ID: 128, Partition: [43, 45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                              Node ID: 43, Partition: [43]\n",
      "                                                                                                              Node ID: 127, Partition: [45, 27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                  Node ID: 45, Partition: [45]\n",
      "                                                                                                                  Node ID: 126, Partition: [27, 87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                      Node ID: 27, Partition: [27]\n",
      "                                                                                                                      Node ID: 125, Partition: [87, 11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                          Node ID: 124, Partition: [11, 29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                             Node ID: 11, Partition: [11]\n",
      "                                                                                                                             Node ID: 123, Partition: [29, 70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                 Node ID: 122, Partition: [70, 93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                    Node ID: 121, Partition: [93, 15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                       Node ID: 120, Partition: [15, 55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                          Node ID: 15, Partition: [15]\n",
      "                                                                                                                                          Node ID: 119, Partition: [55, 47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                              Node ID: 118, Partition: [47, 63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                 Node ID: 117, Partition: [63, 79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                    Node ID: 116, Partition: [79, 98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                       Node ID: 115, Partition: [98, 78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                          Node ID: 98, Partition: [98]\n",
      "                                                                                                                                                          Node ID: 114, Partition: [78, 35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                              Node ID: 113, Partition: [35, 31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                 Node ID: 112, Partition: [31, 89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                    Node ID: 31, Partition: [31]\n",
      "                                                                                                                                                                    Node ID: 111, Partition: [89, 42, 49, 39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                        Node ID: 101, Partition: [89, 42, 49]\n",
      "                                                                                                                                                                           Node ID: 89, Partition: [89]\n",
      "                                                                                                                                                                           Node ID: 100, Partition: [42, 49]\n",
      "                                                                                                                                                                               Node ID: 49, Partition: [49]\n",
      "                                                                                                                                                                               Node ID: 42, Partition: [42]\n",
      "                                                                                                                                                                        Node ID: 110, Partition: [39, 53, 67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                            Node ID: 109, Partition: [67, 86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                               Node ID: 67, Partition: [67]\n",
      "                                                                                                                                                                               Node ID: 108, Partition: [86, 16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                                   Node ID: 107, Partition: [16, 75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                                      Node ID: 16, Partition: [16]\n",
      "                                                                                                                                                                                      Node ID: 106, Partition: [75, 52, 71, 7, 40]\n",
      "                                                                                                                                                                                          Node ID: 105, Partition: [52, 71, 7, 40]\n",
      "                                                                                                                                                                                             Node ID: 104, Partition: [71, 7, 40]\n",
      "                                                                                                                                                                                                Node ID: 71, Partition: [71]\n",
      "                                                                                                                                                                                                Node ID: 103, Partition: [7, 40]\n",
      "                                                                                                                                                                                                    Node ID: 40, Partition: [40]\n",
      "                                                                                                                                                                                                    Node ID: 7, Partition: [7]\n",
      "                                                                                                                                                                                             Node ID: 52, Partition: [52]\n",
      "                                                                                                                                                                                          Node ID: 75, Partition: [75]\n",
      "                                                                                                                                                                                   Node ID: 86, Partition: [86]\n",
      "                                                                                                                                                                            Node ID: 102, Partition: [39, 53]\n",
      "                                                                                                                                                                                Node ID: 53, Partition: [53]\n",
      "                                                                                                                                                                                Node ID: 39, Partition: [39]\n",
      "                                                                                                                                                                 Node ID: 35, Partition: [35]\n",
      "                                                                                                                                                              Node ID: 78, Partition: [78]\n",
      "                                                                                                                                                       Node ID: 79, Partition: [79]\n",
      "                                                                                                                                                    Node ID: 63, Partition: [63]\n",
      "                                                                                                                                                 Node ID: 47, Partition: [47]\n",
      "                                                                                                                                              Node ID: 55, Partition: [55]\n",
      "                                                                                                                                       Node ID: 93, Partition: [93]\n",
      "                                                                                                                                    Node ID: 70, Partition: [70]\n",
      "                                                                                                                                 Node ID: 29, Partition: [29]\n",
      "                                                                                                                          Node ID: 87, Partition: [87]\n",
      "                                                                                                           Node ID: 10, Partition: [10]\n",
      "                                                                                                    Node ID: 23, Partition: [23]\n",
      "                                                                                             Node ID: 5, Partition: [5]\n",
      "                                                                              Node ID: 25, Partition: [25]\n",
      "                                                                           Node ID: 138, Partition: [60, 65]\n",
      "                                                                               Node ID: 65, Partition: [65]\n",
      "                                                                               Node ID: 60, Partition: [60]\n",
      "                                                                        Node ID: 140, Partition: [4, 30]\n",
      "                                                                            Node ID: 4, Partition: [4]\n",
      "                                                                            Node ID: 30, Partition: [30]\n",
      "                                                             Node ID: 147, Partition: [85, 24, 36]\n",
      "                                                                 Node ID: 146, Partition: [24, 36]\n",
      "                                                                    Node ID: 24, Partition: [24]\n",
      "                                                                    Node ID: 36, Partition: [36]\n",
      "                                                                 Node ID: 85, Partition: [85]\n",
      "                                                          Node ID: 143, Partition: [2, 69]\n",
      "                                                              Node ID: 2, Partition: [2]\n",
      "                                                              Node ID: 69, Partition: [69]\n",
      "                                                       Node ID: 151, Partition: [57, 56, 81]\n",
      "                                                           Node ID: 57, Partition: [57]\n",
      "                                                           Node ID: 150, Partition: [56, 81]\n",
      "                                                               Node ID: 56, Partition: [56]\n",
      "                                                               Node ID: 81, Partition: [81]\n",
      "                                                    Node ID: 155, Partition: [48, 95, 68, 72]\n",
      "                                                        Node ID: 48, Partition: [48]\n",
      "                                                        Node ID: 154, Partition: [95, 68, 72]\n",
      "                                                            Node ID: 153, Partition: [68, 72]\n",
      "                                                               Node ID: 72, Partition: [72]\n",
      "                                                               Node ID: 68, Partition: [68]\n",
      "                                                            Node ID: 95, Partition: [95]\n",
      "                                                 Node ID: 158, Partition: [97, 41, 92]\n",
      "                                                     Node ID: 97, Partition: [97]\n",
      "                                                     Node ID: 157, Partition: [41, 92]\n",
      "                                                         Node ID: 41, Partition: [41]\n",
      "                                                         Node ID: 92, Partition: [92]\n",
      "                                          Node ID: 162, Partition: [13, 77]\n",
      "                                              Node ID: 13, Partition: [13]\n",
      "                                              Node ID: 77, Partition: [77]\n",
      "                                       Node ID: 164, Partition: [37, 80]\n",
      "                                           Node ID: 80, Partition: [80]\n",
      "                                           Node ID: 37, Partition: [37]\n",
      "                                    Node ID: 167, Partition: [14, 46, 84]\n",
      "                                        Node ID: 166, Partition: [46, 84]\n",
      "                                           Node ID: 84, Partition: [84]\n",
      "                                           Node ID: 46, Partition: [46]\n",
      "                                        Node ID: 14, Partition: [14]\n",
      "                                 Node ID: 170, Partition: [9, 22, 88]\n",
      "                                     Node ID: 9, Partition: [9]\n",
      "                                     Node ID: 169, Partition: [22, 88]\n",
      "                                         Node ID: 88, Partition: [88]\n",
      "                                         Node ID: 22, Partition: [22]\n",
      "                              Node ID: 173, Partition: [99, 8, 74]\n",
      "                                  Node ID: 99, Partition: [99]\n",
      "                                  Node ID: 172, Partition: [8, 74]\n",
      "                                      Node ID: 8, Partition: [8]\n",
      "                                      Node ID: 74, Partition: [74]\n",
      "                       Node ID: 180, Partition: [51, 64, 83]\n",
      "                           Node ID: 51, Partition: [51]\n",
      "                           Node ID: 179, Partition: [64, 83]\n",
      "                               Node ID: 64, Partition: [64]\n",
      "                               Node ID: 83, Partition: [83]\n",
      "                Node ID: 194, Partition: [54, 73, 59, 90, 32, 18, 33, 76, 91]\n",
      "                    Node ID: 193, Partition: [73, 59, 90, 32, 18, 33, 76, 91]\n",
      "                       Node ID: 192, Partition: [59, 90, 32, 18, 33, 76, 91]\n",
      "                          Node ID: 190, Partition: [59, 90, 32, 18, 33]\n",
      "                             Node ID: 187, Partition: [59, 90]\n",
      "                                Node ID: 90, Partition: [90]\n",
      "                                Node ID: 59, Partition: [59]\n",
      "                             Node ID: 189, Partition: [32, 18, 33]\n",
      "                                 Node ID: 32, Partition: [32]\n",
      "                                 Node ID: 188, Partition: [18, 33]\n",
      "                                     Node ID: 33, Partition: [33]\n",
      "                                     Node ID: 18, Partition: [18]\n",
      "                          Node ID: 191, Partition: [76, 91]\n",
      "                              Node ID: 91, Partition: [91]\n",
      "                              Node ID: 76, Partition: [76]\n",
      "                       Node ID: 73, Partition: [73]\n",
      "                    Node ID: 54, Partition: [54]\n",
      "         Node ID: 44, Partition: [44]\n"
     ]
    }
   ],
   "source": [
    "# Print the tree starting from the root node\n",
    "print(\"Partition Tree Structure:\")\n",
    "print_tree_ascii(root_id, tree_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f499683-2de5-4148-a51c-e3d17b8d44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Function to plot the dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "    # Plot the dendrogram\n",
    "    dendrogram(linkage_matrix, labels=labels, **kwargs)\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.title('Hierarchical Clustering Dendrogram with Constraints')\n",
    "plot_dendrogram(clustering, orientation='top')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20516dc1-c63f-4a14-8cd6-97938cd43b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
